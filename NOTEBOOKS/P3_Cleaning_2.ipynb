{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v5po1IBMIcSj"
   },
   "source": [
    "# IMPORT BACK THE SESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5774,
     "status": "ok",
     "timestamp": 1587551476058,
     "user": {
      "displayName": "Maryse Muller",
      "photoUrl": "",
      "userId": "13230828717959946182"
     },
     "user_tz": -120
    },
    "id": "5dxw4VBWZsPO",
    "outputId": "c892b915-d0f6-4b7a-e570-72e46da45a6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-stdnum\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/57/5d1ee250a9c4e780d57d7dcc1b9553c4e386c759ed3087f813965c4534d2/python_stdnum-1.13-py2.py3-none-any.whl (839kB)\n",
      "\r",
      "\u001b[K     |▍                               | 10kB 26.2MB/s eta 0:00:01\r",
      "\u001b[K     |▉                               | 20kB 2.2MB/s eta 0:00:01\r",
      "\u001b[K     |█▏                              | 30kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█▋                              | 40kB 2.1MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 51kB 2.3MB/s eta 0:00:01\r",
      "\u001b[K     |██▍                             | 61kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |██▊                             | 71kB 3.0MB/s eta 0:00:01\r",
      "\u001b[K     |███▏                            | 81kB 3.2MB/s eta 0:00:01\r",
      "\u001b[K     |███▌                            | 92kB 3.6MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 102kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |████▎                           | 112kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 122kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 133kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 143kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████▉                          | 153kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 163kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████▋                         | 174kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 184kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████▍                        | 194kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 204kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████▏                       | 215kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████▋                       | 225kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 235kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▍                      | 245kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▊                      | 256kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▏                     | 266kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▌                     | 276kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 286kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▎                    | 296kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▊                    | 307kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 317kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 327kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▉                   | 337kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▎                  | 348kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▋                  | 358kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 368kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▍                 | 378kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 389kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 399kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▋                | 409kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 419kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▍               | 430kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▉               | 440kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▏              | 450kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▋              | 460kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 471kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▍             | 481kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▊             | 491kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▏            | 501kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▌            | 512kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 522kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▎           | 532kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▊           | 542kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 552kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▌          | 563kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▉          | 573kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 583kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▋         | 593kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 604kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 614kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▉        | 624kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▏       | 634kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▋       | 645kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 655kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▍      | 665kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▊      | 675kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 686kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 696kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 706kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▎    | 716kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 727kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 737kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▌   | 747kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 757kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▎  | 768kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▋  | 778kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 788kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▍ | 798kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▉ | 808kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 819kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 829kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 839kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 849kB 3.4MB/s \n",
      "\u001b[?25hInstalling collected packages: python-stdnum\n",
      "Successfully installed python-stdnum-1.13\n"
     ]
    }
   ],
   "source": [
    "# Installation in Colab environment only\n",
    "!pip install python-stdnum\n",
    "\n",
    "# importation\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.colab import files\n",
    "from stdnum.exceptions import *\n",
    "from stdnum.util import *\n",
    "from stdnum.ean import *\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact, interact_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EMJPW7oNiDuv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 57,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3917,
     "status": "ok",
     "timestamp": 1587550794160,
     "user": {
      "displayName": "Maryse Muller",
      "photoUrl": "",
      "userId": "13230828717959946182"
     },
     "user_tz": -120
    },
    "id": "BPeZKyXPLLqo",
    "outputId": "69c74582-1bcf-448d-bb3d-24d0f04cc787"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-4464119b-d1b2-494f-9606-41312635884a\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-4464119b-d1b2-494f-9606-41312635884a\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files.upload() # OFF_up_to_1_3.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4WpsRNL1QbuW"
   },
   "outputs": [],
   "source": [
    "f = open('d_types.txt', 'r')\n",
    "dict_str = f.read()\n",
    "d_types = json.loads(dict_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Oeiv1yxIhEZ"
   },
   "outputs": [],
   "source": [
    "dfcopy = pd.read_csv('/content/OFF_up_to_1_3.csv',\n",
    "                     dtype=d_types, low_memory=False, encoding ='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a1PoAfg4Z5o2"
   },
   "outputs": [],
   "source": [
    "# print(dfcopy.info(memory_usage='deep'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "drBGqvt0_KI7"
   },
   "source": [
    "  ### 1.4 Dropping inoperable rows\n",
    "\n",
    "We need to get rid of inoperable rows. We will define operability of a row as follows:\n",
    "\n",
    "- condition 1: the product can be identified, at least by a unique combination of name, brand and quantity ('product_name', 'quantity', 'brands'), OR by a valid and unique barcode ('code')\n",
    "\n",
    "AND\n",
    "- condition 2: at least one numerical information is provided i.e. one of the features 'XXX_100g' or one type of nutritional score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Fhqy5F1AiGS"
   },
   "source": [
    "#### *Condition 1*:\n",
    "\n",
    "Let's first check the 'code' column for completion and validity of the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 34785,
     "status": "ok",
     "timestamp": 1587552158880,
     "user": {
      "displayName": "Maryse Muller",
      "photoUrl": "",
      "userId": "13230828717959946182"
     },
     "user_tz": -120
    },
    "id": "dtMpEDuyEnKP",
    "outputId": "4ddcddda-7537-4a64-cb8e-26d223c1dac0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking nb of null values in 'code' column\n",
    "dfcopy['code'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6vjbctmgEnhC"
   },
   "source": [
    "No null value in the 'code' column.\n",
    "\n",
    "We will now create a new column 'code_val' indicating if the barcode is valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 51571,
     "status": "ok",
     "timestamp": 1587552175696,
     "user": {
      "displayName": "Maryse Muller",
      "photoUrl": "",
      "userId": "13230828717959946182"
     },
     "user_tz": -120
    },
    "id": "cKMtX9TgqTTC",
    "outputId": "6db8ba79-90d9-4f7d-96d4-7bdf0a95eb8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb of valid codes:  1192439\n",
      "nb of invalid codes:  23212\n",
      "pctage of valid codes:  98.09057040219602 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'nb of valid codes:  1190822\\nnb of invalid codes:  23153\\npctage of valid codes:  98.09279433266748 %'"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verification of barcodes (EAN-13, EAN-8 and UPC (12-digit) format)\n",
    "# Creation of a new column : barcode valid or not\n",
    "dfcopy.insert(1,'code_val', dfcopy['code'].apply(is_valid))\n",
    "print(\"nb of valid codes: \", dfcopy['code_val'].sum())\n",
    "print(\"nb of invalid codes: \", (~dfcopy['code_val']).sum())\n",
    "print(\"pctage of valid codes: \", dfcopy['code_val'].sum()*100/dfcopy.shape[0], \"%\")\n",
    "\n",
    "'''nb of valid codes:  1190822\n",
    "nb of invalid codes:  23153\n",
    "pctage of valid codes:  98.09279433266748 %'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-TPRGLC0ECGD"
   },
   "source": [
    "When the code is not valid, we will check if the product can be identified by a unique combination of \"product_name\", \"quantity\", \"brands\"  that allow to detect possible duplicates. (At least one of the three must be filled i.e. not NaN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j9yz1G1cCuSO"
   },
   "outputs": [],
   "source": [
    "key_cols = ['product_name', 'quantity', 'brands']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 51804,
     "status": "ok",
     "timestamp": 1587552175964,
     "user": {
      "displayName": "Maryse Muller",
      "photoUrl": "",
      "userId": "13230828717959946182"
     },
     "user_tz": -120
    },
    "id": "qesuqy5zCujc",
    "outputId": "bd256154-4ba0-45c8-cde3-75202310d34d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, [])"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for invalid 'product_name' (less than 2 characters)\n",
    "df_prod_notna = pd.DataFrame(dfcopy['product_name'].dropna())\n",
    "li_ind_all = df_prod_notna.index.to_list()\n",
    "li_len_prod = list(map(len, df_prod_notna))\n",
    "li_ind_len = list(zip(li_ind_all,li_len_prod))\n",
    "ind_wrong_names = [ind for ind, length in li_ind_len if length < 2]\n",
    "len(ind_wrong_names), dfcopy.loc[ind_wrong_names]['product_name'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZufbWRGDU39d"
   },
   "outputs": [],
   "source": [
    "# Replacing invalid names with np.NaN\n",
    "dfcopy.loc[ind_wrong_names,['product_name']] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1000,
     "status": "ok",
     "timestamp": 1587553012126,
     "user": {
      "displayName": "Maryse Muller",
      "photoUrl": "",
      "userId": "13230828717959946182"
     },
     "user_tz": -120
    },
    "id": "LxL7gh66TbRQ",
    "outputId": "baa081d9-1aed-4a67-f12b-17e368c8ccd2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, [], Categories (0, object): [])"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for 'brands' (less than 1 characters)\n",
    "df_brands_notna = dfcopy['brands'].dropna()\n",
    "li_ind_all = df_brands_notna.index.to_list()\n",
    "li_len_brands = list(map(len, df_brands_notna))\n",
    "li_ind_len = list(zip(li_ind_all,li_len_brands))\n",
    "ind_wrong_names = [ind for ind, length in li_ind_len if length < 1]\n",
    "len(ind_wrong_names), dfcopy.loc[ind_wrong_names]['brands'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l2gjTrqJCuun"
   },
   "outputs": [],
   "source": [
    "### Function decomposing 'quantity' in groups of (numerical value, unit)\n",
    "\n",
    "# To ensure safe execution of function 'float'\n",
    "def safe_exe(def_val, function, *args):\n",
    "    try: return function(*args)\n",
    "    except: return def_val\n",
    "\n",
    "# Conversion of string in float if possible, else 0 (accepts 'n1 x n2' string)\n",
    "def conv_float(my_str):\n",
    "  idx = 0\n",
    "  if 'x' in my_str:\n",
    "    idx = my_str.find('x')\n",
    "    n1 = safe_exe(0, float, my_str[:idx])\n",
    "    n2 = safe_exe(0, float, my_str[idx+1:])\n",
    "    return n1*n2\n",
    "  else:\n",
    "    return safe_exe(0, float, my_str)\n",
    "\n",
    "def num_units(my_str):\n",
    "  my_str = my_str.lower().strip()\n",
    "  regex = r'([0-9.,x ]*)\\s*([^()0-9 !,\\-±=\\*\\+/.-\\?\\[\\]]*\\s*)'\n",
    "  res = re.findall(regex, my_str)\n",
    "  res.remove(('', ''))\n",
    "  num = [conv_float(gr[0].replace(' ','').replace(',','.')) for gr in res]\n",
    "  unit = [gr[1].strip()  for gr in res]\n",
    "  res=list(zip(num,unit))\n",
    "  return num, unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 601
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1073,
     "status": "error",
     "timestamp": 1587553198753,
     "user": {
      "displayName": "Maryse Muller",
      "photoUrl": "",
      "userId": "13230828717959946182"
     },
     "user_tz": -120
    },
    "id": "6FICcd28Du37",
    "outputId": "d97d122f-ebff-4075-fabf-545accb3fa3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb of row with non empty 'quantity': 379515  on  1215651\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, categories, ordered, dtype, fastpath)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                 \u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mfactorize\u001b[0;34m(values, sort, na_sentinel, size_hint)\u001b[0m\n\u001b[1;32m    635\u001b[0m         codes, uniques = _factorize_array(\n\u001b[0;32m--> 636\u001b[0;31m             \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_sentinel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_sentinel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_hint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_hint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36m_factorize_array\u001b[0;34m(values, na_sentinel, size_hint, na_value)\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhash_klass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_hint\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m     \u001b[0muniques\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_sentinel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_sentinel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.factorize\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable._unique\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-b37bbebfa920>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nb of row with non empty 'quantity':\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_quant_notna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" on \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf_quantity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'analyse'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_quant_notna\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'quantity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# df_quantity['num_gr'] = [t[0] for t in df_quantity['analyse']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# df_quantity['unit_gr'] = [t[1] for t in df_quantity['analyse']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3843\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"map\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3844\u001b[0m                 \u001b[0;31m# GH#23179 some EAs do not have `map`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3845\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, mapper)\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'first'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'second'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnan\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m         \"\"\"\n\u001b[0;32m-> 1199\u001b[0;31m         \u001b[0mnew_categories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m             return self.from_codes(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, mapper, na_action)\u001b[0m\n\u001b[1;32m   4574\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4575\u001b[0m                 \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4576\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4578\u001b[0m         \u001b[0mattributes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"copy\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36mfrom_tuples\u001b[0;34m(cls, tuples, sortorder, names)\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msortorder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msortorder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36mfrom_arrays\u001b[0;34m(cls, arrays, sortorder, names)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"all arrays must be same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactorize_from_iterables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36mfactorize_from_iterables\u001b[0;34m(iterables)\u001b[0m\n\u001b[1;32m   2706\u001b[0m         \u001b[0;31m# For consistency, it should return a list of 2 lists.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2707\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2708\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactorize_from_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2706\u001b[0m         \u001b[0;31m# For consistency, it should return a list of 2 lists.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2707\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2708\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactorize_from_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36mfactorize_from_iterable\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   2678\u001b[0m         \u001b[0;31m# but only the resulting categories, the order of which is independent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2679\u001b[0m         \u001b[0;31m# from ordered. Set ordered to False as default. See GH #15457\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2680\u001b[0;31m         \u001b[0mcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mordered\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2681\u001b[0m         \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2682\u001b[0m         \u001b[0mcodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, categories, ordered, dtype, fastpath)\u001b[0m\n\u001b[1;32m    355\u001b[0m                 \u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m                 \u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mordered\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m                     \u001b[0;31m# raise, as we don't have a sortable data structure and so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mfactorize\u001b[0;34m(values, sort, na_sentinel, size_hint)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m         codes, uniques = _factorize_array(\n\u001b[0;32m--> 636\u001b[0;31m             \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_sentinel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_sentinel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_hint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_hint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m         )\n\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36m_factorize_array\u001b[0;34m(values, na_sentinel, size_hint, na_value)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhash_klass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_hint\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m     \u001b[0muniques\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_sentinel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_sentinel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[0mcodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_platform_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.factorize\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable._unique\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# Creating a new database with the main features of the 'quantity' data\n",
    "# (avoiding rows with empty 'quantity')\n",
    "\n",
    "df_quant_notna = dfcopy.dropna(subset=['quantity'])\n",
    "df_quantity = pd.DataFrame([])\n",
    "df_quantity['quantity'] = df_quant_notna['quantity']\n",
    "print(\"nb of row with non empty 'quantity':\", df_quant_notna.shape[0],\" on \", dfcopy.shape[0])\n",
    "\n",
    "df_quantity['analyse'] = df_quant_notna['quantity'].apply(num_units)\n",
    "# df_quantity['num_gr'] = [t[0] for t in df_quantity['analyse']]\n",
    "# df_quantity['unit_gr'] = [t[1] for t in df_quantity['analyse']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B8bWnfFdeEll"
   },
   "outputs": [],
   "source": [
    "# Displaying the main units in 'quantity'\n",
    "units = df_quantity['unit_gr']\n",
    "all_units = units.values\n",
    "all_units = [x[i] for x in all_units for i in range(len(x))]\n",
    "print(\"Total nb of not NaN rows: \", units.shape[0])\n",
    "print(\"Total nb of identified units: \", len(all_units))\n",
    "print(\"Nb of unique units: \" , len(list(set(all_units))))\n",
    "print(\"List of unique units: \\n\", list(set(all_units)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QGoxKBIbKPzp"
   },
   "outputs": [],
   "source": [
    "# Classifying main relevant units in 'mass' and 'volume' units\n",
    "li_u_mass = ['g', 'kg', 'gr', 'grammes', 'grs','st', 'mg', 'gramm', 'lb','gram',\n",
    "             'grams', 'gramos', 'lbs', 'gm', 'lt', 'lts','gramme', 'kilo','公克',\n",
    "             'grammi', 'kgs', 'kgr', 'gms', 'g-', 'grms','pound', 'pounds', \n",
    "             'grm', 'grames','غرام', 'جرام','غ', 'غم','جم','g℮', 'г', 'кг', '克', \n",
    "             'грамм', 'גרם','kilogramm','gramas','kilogrammae','livres',\n",
    "             'grame', 'kilos'] \n",
    "li_u_vol = ['ml','dl','l','cl', 'oz', 'litre', 'fl', 'litres', 'liter','litro',\n",
    "            'litri','litr', 'ounces','ounce', 'ltr', 'gallon','half-gallon',\n",
    "            'litros','litroe', 'liters', 'cc', 'kl', 'pint','pints', 'gal',\n",
    "            'mls', 'centilitres', 'لتر','مل','ل','ليتر', 'มล', 'ลิตร', 'мл', 'л',\n",
    "            'litrè', 'milliliter','millilitre', 'γρ', 'литр', 'литра', 'mml',\n",
    "            'מ״ל','millilitres','λίτρο', 'mĺ', 'cm', 'cm³' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DEVV_OztoiqN"
   },
   "outputs": [],
   "source": [
    "# Function selecting the most relevant couple (num,unit)\n",
    "# by order of priority : unit in li_prio1, then in li_prio2\n",
    "def sel_gr(li, li_prio1, li_prio2):\n",
    "  res = 0\n",
    "  nums = li[0]\n",
    "  units = li[1]\n",
    "  tab_t_u = []\n",
    "  ind = np.nan\n",
    "  for i in units:\n",
    "    if  i in li_prio1:\n",
    "      tab_t_u.append(2)\n",
    "    elif  i in li_prio2:\n",
    "      tab_t_u.append(1)\n",
    "    else:\n",
    "      tab_t_u.append(0) if i!='' else tab_t_u.append(np.nan)\n",
    "  itab1 = [i for i in range(len(tab_t_u)) if tab_t_u[i]==2] # indexes of all volumes (prio1)\n",
    "  itab2 = [i for i in range(len(tab_t_u)) if tab_t_u[i]==1] # indexes of all masses (prio2)\n",
    "  itab0 = [i for i in range(len(tab_t_u)) if tab_t_u[i]==0] # indexes of all others (prio3)\n",
    "  if len(itab1)>0: # prio1 (vol)\n",
    "    ind = itab1[np.argmax([nums[i] for i in itab1])]\n",
    "  elif len(itab2)>0: # prio2 (mass)\n",
    "    ind = itab2[np.argmax([nums[i] for i in itab2])]\n",
    "  else:\n",
    "    ind = itab0[np.argmax([nums[i] for i in itab0])] if len(itab0)>0 else np.nan\n",
    "  return (nums[ind], units[ind]) if ind is not np.nan else (np.nan, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IsFMysvjDS2j"
   },
   "outputs": [],
   "source": [
    "my_fun = lambda x:sel_gr(x, li_u_vol, li_u_mass)\n",
    "li_best_num_unit = df_quantity['analyse'].apply(my_fun)\n",
    "\n",
    "df_quantity['num'] = [gr1 for gr1, gr2 in li_best_num_unit]\n",
    "df_quantity['unit'] = [gr2 for gr1, gr2 in li_best_num_unit]\n",
    "\n",
    "df_quantity['nb_groups'] = [len(gr[0]) for gr in df_quantity['analyse']]\n",
    "df_quantity['nb_char'] = [len(s) if s is not np.nan else 0 for s in df_quantity['quantity']]\n",
    "df_quantity['nb_char_unit'] = [len(s) if s is not np.nan else 0 for s in df_quantity['unit']]\n",
    "\n",
    "df_quantity['unit_bool'] = [(1 if n!=0 else 0) for n in df_quantity['nb_char_unit']]\n",
    "df_quantity['num_bool'] = [(1 if n!=0 else 0)for n in df_quantity['num']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fDNkGLQoMDUS"
   },
   "outputs": [],
   "source": [
    "# Function that links 'mass','volume'and 'other' to numerical values (1,2,0)\n",
    "def quantity_type(my_string):\n",
    "  if my_string is None:\n",
    "    return np.nan\n",
    "  else:\n",
    "    return 1 if my_string in li_u_mass else 2 if my_string in li_u_vol else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l8cNW2dluSk4"
   },
   "outputs": [],
   "source": [
    "# New column identifying 'mass' (1), 'volume' (2) and 'other' (0)\n",
    "i = df_quantity.columns.to_list().index('unit') + 1 # after 'unit'\n",
    "df_quantity.insert(i, 'q_unit_type',\n",
    "                   [quantity_type(s) if s is not np.nan else np.nan \\\n",
    "                    for s in df_quantity['unit']])\n",
    "df_quantity.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BojXKaj-ObSw"
   },
   "outputs": [],
   "source": [
    "# Pick one of each randomly to display\n",
    "\n",
    "idx=[]\n",
    "sel_idx=[]\n",
    "my_dict=dict(zip([0,1,2,3],[0,1,2,np.nan]))\n",
    "cpt=0\n",
    "for i in my_dict:\n",
    "  if my_dict[i] is not np.nan:\n",
    "    idx.append(df_quantity[df_quantity['q_unit_type']==my_dict[i]].index.to_list())\n",
    "  else :\n",
    "    idx.append(df_quantity[df_quantity['q_unit_type'].isna()].index.to_list())\n",
    "  r = randint(0,len(idx[cpt]))\n",
    "  sel_idx.append(idx[cpt][r])\n",
    "  cpt+=1\n",
    "\n",
    "print(\"mass : \",len(idx[1]), \"volume : \",len(idx[2]),\\\n",
    "      \"other : \",len(idx[0]),\"nan : \",len(idx[3]),\\\n",
    "      \"Total : \", df_quantity.shape[0])\n",
    "df_quantity.loc[sel_idx,['quantity' ,'num' ,'unit' , 'q_unit_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uE1P0mprVL4p"
   },
   "outputs": [],
   "source": [
    "# Defining limits to facilitate clustering algorithm\n",
    "# despite outliers in 'num' values\n",
    "(num_min, num_max) = (0,10000) #(mean-2*std, mean+2*std) #(0,10000)\n",
    "# Creating a filtered version of df_quantity without outliers \n",
    "df_quantity_f = df_quantity[(df_quantity['num']<num_max)&\\\n",
    "                            (df_quantity['num']>num_min)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oLQ_9L2lzjWO"
   },
   "outputs": [],
   "source": [
    "# Statistics to assess dispersion of 'num_max' data\n",
    "(mean, std) = df_quantity['num'].mean(), df_quantity['num'].std()\n",
    "(median, q_01, q_99) = df_quantity['num'].median(),\\\n",
    "                       df_quantity['num'].quantile(0.01),\\\n",
    "                       df_quantity['num'].quantile(0.99)\n",
    "print(\"Nb rows in original df vs filtered df:\\n\",\n",
    "      (df_quantity.shape[0], df_quantity_f.shape[0]) )\n",
    "print(\"Mean and standard error of 'unit' column:\\n\", (mean, std))\n",
    "print(\"1st centile, median and 99h centile:\\n\", (q_01, median, q_99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UlVHQkyE7OXh"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "n_clust = 7\n",
    "X = df_quantity_f[['nb_groups',\t'nb_char', 'num',\t'nb_char_unit',\\\n",
    "                   'q_unit_type','num_bool',\t'unit_bool']]\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "kmeans = KMeans(n_clusters=n_clust, random_state=0).fit(X)\n",
    "df_quantity_f['cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gtjs7LSOzuyd"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (21, 6))\n",
    "grid = plt.GridSpec(1, 1, wspace=0.2, hspace=0.1)\n",
    "# grid = plt.GridSpec(1, 3, wspace=0.2, hspace=0.1)\n",
    "np.warnings.filterwarnings('ignore')\n",
    "\n",
    "mkr_dict = {'mass' : (1,'s'), 'volume' : (2,'o'), 'other' : (0,'*')}\n",
    "\n",
    "ax1 = plt.subplot(grid[0, 0:1])\n",
    "for mkr in mkr_dict:\n",
    "  mask = df_quantity_f['q_unit_type'] == mkr_dict[mkr][0]\n",
    "  log_num_max = 100*np.log(df_quantity_f[mask]['num'])\n",
    "  df_quantity_f[mask].plot(ax = ax1, kind = 'scatter',\n",
    "                           s = df_quantity_f[mask]['nb_char'],\n",
    "                           y='nb_groups',x='num', c='cluster',\n",
    "                           marker = mkr_dict[mkr][1], colormap='viridis',\n",
    "                           xlim = (0,3000), ylim = (0,20), logx = True,\n",
    "                           alpha=0.3, legend = False,) # figsize = (18,6))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A9Kze8bpRZhW"
   },
   "outputs": [],
   "source": [
    "### Function giving description of data in a cluster\n",
    "def analyses_cluster (df,n):\n",
    "  df_cl = df[df['cluster']==n]\n",
    "  print(\"o---- CLUSTER \", str(n), \" ----o\")\n",
    "  print(\"nb rows\", df_cl.shape[0])\n",
    "  print(\"min and max num: \", df_cl['num'].min(), df_cl['num'].max())\n",
    "  print(\"nb of empty unit: \", df_cl[(df_cl['unit']=='')].shape[0])\n",
    "  print(df_cl['nb_groups'].value_counts())\n",
    " # print(\"----------- Data in the cluster with empty unit_max : --------\")\n",
    " # return df_cl[(df_cl['unit_max']=='')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xrhbYkEr7OkI"
   },
   "outputs": [],
   "source": [
    "# Checking the repartition of rows between clusters\n",
    "for i in range(n_clust):\n",
    "  print(\"cluster \"+str(i)+\" :\",df_quantity_f[df_quantity_f['cluster']==i].shape)\n",
    "# Then giving a summary of main characteristics\n",
    "for i in range(n_clust):\n",
    "  analyses_cluster(df_quantity_f,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ThBfNNuOZmVN"
   },
   "outputs": [],
   "source": [
    "# indexes of rows with empty 'unit'\n",
    "idx_unit_na = df_quantity[df_quantity['unit'].isna()].index.to_list()\n",
    "print(\"Found {} empty 'unit'.\\n\".format(len(idx_unit_na)))\n",
    "\n",
    "# indexes of rows with 'num' 0, negative or values exceeding 6000\n",
    "cond1 = df_quantity['num'].isna()\n",
    "cond2 = df_quantity['num']>10000\n",
    "cond3 = df_quantity['num']<=0\n",
    "my_cond = (cond1|cond2|cond3)\n",
    "idx_num_outl = df_quantity[my_cond].index.to_list()\n",
    "print(\"Total {} outliers found in 'num'.\".format(len(idx_num_outl)))\n",
    "print(\"--> NaN: {0} - Outliers: {1} - Zero or negative: {2}\"\\\n",
    "      .format(df_quantity[cond1].shape[0],\n",
    "              df_quantity[cond2].shape[0],\n",
    "              df_quantity[cond3].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZK8fXpWCNmF"
   },
   "outputs": [],
   "source": [
    "# Dropping rows where unit or num is invalid in df_quantity\n",
    "df_quantity.drop(index=idx_unit_na+idx_num_outl, inplace=True)\n",
    "# Setting value of 'quantity' in dfcopy also to NaN if unit or num is invalid \n",
    "dfcopy.loc[idx_unit_na+idx_num_outl, 'quantity'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mMwjcAdzVVxL"
   },
   "outputs": [],
   "source": [
    "# Merging df_quantity with dfcopy\n",
    "print(\"dfcopy before: \\n\", dfcopy.shape,\n",
    "      \"\\ndf_quantity: \\n\", df_quantity.shape)\n",
    "dfcopy = dfcopy.merge(df_quantity[['num', 'unit','q_unit_type']],\n",
    "                      how = 'left',  left_index=True, right_index=True)\n",
    "dfcopy.rename(columns = {'num':'quantity_num',\n",
    "                         'unit':'quantity_unit',\n",
    "                          'q_unit_type':'quantity_type'},\n",
    "                inplace=True)\n",
    "# ---- NB : join on index works, whereas join on column 'quantity' doesn't.\n",
    "cols = dfcopy.columns.to_list()\n",
    "cols = cols[:9]+cols[-3:]+cols[9:-3]\n",
    "dfcopy = dfcopy[cols]\n",
    "print(\"---> dfcopy after: \\n\", dfcopy.shape)\n",
    "dfcopy.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VsPOvAOooorA"
   },
   "outputs": [],
   "source": [
    "# Checking rows with insufficient identification data\n",
    "# Indexes of rows with invalid codes AND all key_cols NaN\n",
    "key_cols = ['product_name', 'quantity_num','quantity_unit', 'brands']\n",
    "m_wcode = dfcopy['code_val']==False\n",
    "m_wkeys = dfcopy.loc[:,key_cols].isna().all(axis=1)\n",
    "\n",
    "ind_drop = dfcopy[m_wcode&m_wkeys].index.to_list()\n",
    "\n",
    "print(\"nb of rows with invalid code\", m_wcode.sum())\n",
    "print(\"nb of rows with all key-columns NaN:\", m_wkeys.sum())\n",
    "print(\"nb of rows with invalid code AND all key-columns NaN (to drop):\", (m_wcode&m_wkeys).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SqI1OTM92uuF"
   },
   "source": [
    "Data that have no key column filled have very few data anyway.\n",
    "We drop all the 22183 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XFANp5Rg-nV2"
   },
   "outputs": [],
   "source": [
    "# Dropping rows with invalid codes AND all key_cols NaN\n",
    "print(\"nb of rows before:\", dfcopy.shape[0])\n",
    "dfcopy.drop(index=ind_drop, inplace=True)\n",
    "print(\"nb of rows after:\", dfcopy.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6SB8JcN4GwF"
   },
   "source": [
    "#### *Condition 2*:\n",
    "\n",
    "Let's now check for the second condition that is \"at least one numerical columns filled\" among a selection of numerical data OR nutritional score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Pi2RzbXiamY"
   },
   "outputs": [],
   "source": [
    "def nb_rows_allna (df, selcol):\n",
    "  mask = dfcopy[selcol].isna().all(axis=1)\n",
    "  ind_drop = dfcopy[mask].index.to_list()\n",
    "  return len(ind_drop), ind_drop\n",
    "  \n",
    "def print_nb(df, selcol):\n",
    "  print(\"------ columns: \", selcol, \"\\nnb of rows all NaN : \", nb_rows_allna(df, selcol)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gkte5xg6D0rV"
   },
   "outputs": [],
   "source": [
    "list(dfcopy.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n5DzcEQZ8FW7"
   },
   "outputs": [],
   "source": [
    "# Display amount of rows to drop depending on selection of columns considered\n",
    "cruc_num_col = ['energy_100g']\n",
    "print_nb(dfcopy, cruc_num_col)\n",
    "cruc_num_col += li_cat_comp\n",
    "print_nb(dfcopy, cruc_num_col)\n",
    "cruc_num_col += ['salt_100g']\n",
    "print_nb(dfcopy, cruc_num_col)\n",
    "cruc_num_col += li_cat_vit_oligo\n",
    "print_nb(dfcopy, cruc_num_col)\n",
    "cruc_num_col += li_cat_nutri\n",
    "print_nb(dfcopy, cruc_num_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-2Mp0C6lKdZJ"
   },
   "source": [
    "We drop the 184 115 rows that do not have either one of the following 36 numerical '**XXX_100g**' columns:\n",
    "\n",
    "['energy_100g'] + li_cat_comp + ['salt_100g'] + li_cat_vit_oligo\n",
    "\n",
    "However, we keep the data with no nutritional data in **li_cat_nutri** columns, as we are going to try to reconsruct those data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kRqi9NUFKE3T"
   },
   "outputs": [],
   "source": [
    "# Dropping rows with no numerical data filled among 'cruc_num_col' columns\n",
    "cruc_num_col = ['energy_100g'] + li_cat_comp + ['salt_100g'] + li_cat_vit_oligo\n",
    "print(\"nb of rows before:\", dfcopy.shape[0])\n",
    "print(nb_rows_allna(dfcopy, cruc_num_col)[0], \"rows to drop\")\n",
    "dfcopy.drop(index=nb_rows_allna(dfcopy, cruc_num_col)[1], inplace=True)\n",
    "print(\"nb of rows after:\", dfcopy.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q8JZYUNtZOUt"
   },
   "outputs": [],
   "source": [
    "speak('Dropped inoperable rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "caps3AdLoDm0"
   },
   "source": [
    "### 1.5 Dealing with duplicates\n",
    "At first we are going to deal with 'absolute' duplicates (i.e. duplicate on each row). We'll simply keep the first row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uoMB2XuvGedt"
   },
   "outputs": [],
   "source": [
    "# 1 - Checking for duplicates on all columns\n",
    "df_dup_all = dfcopy[dfcopy.duplicated(keep=False)]\n",
    "print(\"Nb of duplicated rows on all columns:\", df_dup_all.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B0x7vOPPCK-h"
   },
   "outputs": [],
   "source": [
    "# -> Keeping the first duplicated row\n",
    "print(\"before\", dfcopy.shape[0])\n",
    "dfcopy = dfcopy[~dfcopy.duplicated(keep='first')]\n",
    "print(\"after\", dfcopy.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BDSTxcdkwNG6"
   },
   "source": [
    "Then, to deal with the duplicates on differents subsets of the columns (['code'] and key_cols), we are going to define a function that combines all the duplicated rows in one. The main row will be the one that has the greater number of unique not null values, and it will be unriched by date from the others rows if available (using pd.DataFrame.combine_first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vv4ckthXXFjr"
   },
   "outputs": [],
   "source": [
    "# Function combining all dupl. rows,\n",
    "# '1st' is the one with highest unique notna val.\n",
    "''' Among each group of duplicates :\n",
    " - finds nb of unique notna for each row\n",
    " - selects the one with the most unique notna\n",
    " - combines the rows (1st is the selected row)\n",
    " - replaces all the rows by combined row\n",
    " - returns nb of grps of dupl., indexes of rows to drop, list of rows for update\n",
    " '''\n",
    "def comb_dup_rows(df, cols):\n",
    "  uniq_notna = lambda x:pd.notna(x.unique()).sum()\n",
    "  df_f = pd.DataFrame({})\n",
    "  l_ind = []\n",
    "  cpt = 0\n",
    "  for k,df_g in df.groupby(by=cols): # loop on all grps of dup.\n",
    "    df_comb = pd.DataFrame({})\n",
    "    ser_gr = df_g.apply(uniq_notna, axis=1)\n",
    "    idx_max = np.argmax(ser_gr)\n",
    "    ind = list(ser_gr.index)\n",
    "    ind.remove(idx_max)\n",
    "    df_comb = df_g.loc[idx_max]\n",
    "    for i in ind: # loop on all dupl. except that with max unique not na\n",
    "      df_comb = df_comb.combine_first(df_g.loc[i])\n",
    "    df_f = df_f.append(df_comb)\n",
    "    l_ind += ind\n",
    "    cpt += 1\n",
    "  df_f.reindex(columns=df.columns) #reorder the columns (bug in pd.concat)\n",
    "  return cpt, l_ind, df_f "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J1UnCLV7yLa6"
   },
   "source": [
    "Let's apply this function to dfcopy, with the subset ['code'] for the duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y5oOB9gxkD_I"
   },
   "outputs": [],
   "source": [
    "# 2 - Checking for duplicates on 'code'\n",
    "df_dup = dfcopy[dfcopy.duplicated(subset=['code'],keep=False)]\n",
    "print(\"Nb of duplicated rows on 'code':\", df_dup.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wq-6ApxrwF0_"
   },
   "outputs": [],
   "source": [
    "# duplicates on 'code' column\n",
    "# -> Keeping the combined row, deleting the others\n",
    "print(\"Shape of dfcopy, before\", df_dup.shape[0])\n",
    "nb_gr, ind_drop, df_dup_f = comb_dup_rows(df_dup, ['code'])\n",
    "df_dup.drop(ind_drop, inplace=True)\n",
    "df_dup.update(df_dup_f, overwrite=True)\n",
    "print(\"Nb of groups of duplicates :\", nb_gr)\n",
    "print(\"Nb of duplicates :\", df_dup.shape[0])\n",
    "print(\"Nb of rows to delete:\", len(ind_drop))\n",
    "print(\"Shape of dfcopy, after\", df_dup.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ds6T-C3Ky2mE"
   },
   "outputs": [],
   "source": [
    "# Changing dfcopy\n",
    "dfcopy.drop(ind_drop, inplace=True)\n",
    "dfcopy.update(df_dup_f, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oB9w0f4DbmeE"
   },
   "outputs": [],
   "source": [
    "speak('dropping duplicates (on column code)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SSkCPap8yhTu"
   },
   "source": [
    "29 duplicates has be deleted, and 29 rows updated.\n",
    "\n",
    "Let's apply this function to dfcopy, with the subset key_cols for the duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CKbuDmj2X2lE"
   },
   "outputs": [],
   "source": [
    "dfcopy.groupby(['code_val']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ke3vIY_2zQgf"
   },
   "outputs": [],
   "source": [
    "# 3 - Checking for duplicates on key_cols\n",
    "key_cols = ['product_name', 'quantity_num', 'quantity_unit', 'brands']\n",
    "m_dup_keys = dfcopy.duplicated(subset=key_cols,keep=False)\n",
    "m_c_inval = dfcopy['code_val']==False\n",
    "df_dup_k_c_val = dfcopy[m_dup_keys & ~m_c_inval]\n",
    "df_dup_k_c_inval= dfcopy[m_dup_keys & m_c_inval]\n",
    "print(f\"Nb of duplicated rows on key_cols with VALID code: {df_dup_k_c_val.shape[0]}\" )\n",
    "print(f\"Nb of duplicated rows on key_cols with INVALID code: {df_dup_k_c_inval.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D0UrJIjNqqwX"
   },
   "source": [
    "If there is duplicates with different code, and at least one of them is valide and one invalid, we want to keep only the rows with valide codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_mawCkUBh2AB"
   },
   "outputs": [],
   "source": [
    "# Finding duplicates with at least 1 row with valid code AND 1 invalid code\n",
    "df_mixed_dup = dfcopy[m_dup_keys].groupby(by=key_cols)\\\n",
    "                .filter(lambda x:sum((x['code_val']==True).values)>1\\\n",
    "                        and sum((x['code_val']==False).values)>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "51YAy-0sr3_s"
   },
   "outputs": [],
   "source": [
    "# df_mixed_dup.sort_values(by=key_cols)\n",
    "gb = df_mixed_dup.groupby(by=key_cols)\n",
    "print(f\"Nb of rows with mixed val/inval codes: {sum(gb.size())}\")\n",
    "print(f\"Nb of groups of duplicates: {len(gb)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3dzxaxA8s0Av"
   },
   "outputs": [],
   "source": [
    "# shows example of duplicates on key_cols with mixed 'code_val'\n",
    "gb.get_group(list(gb.groups.keys())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7pWMV_adGmZk"
   },
   "outputs": [],
   "source": [
    "# Dropping the rows with invalid codes in each group\n",
    "print(\"Shape of dfcopy, before\", dfcopy.shape[0])\n",
    "for i in range(len(gb)):\n",
    "  d = gb.get_group(list(gb.groups.keys())[i])\n",
    "  ind = d[d['code_val']==False].index\n",
    "  dfcopy.drop(index=ind, inplace=True)\n",
    "print(\"Shape of dfcopy, after\", dfcopy.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-s-1LNxmzcwo"
   },
   "outputs": [],
   "source": [
    "# Refreshing the list\n",
    "m_dup_keys = dfcopy.duplicated(subset=key_cols,keep=False)\n",
    "m_c_inval = dfcopy['code_val']==False\n",
    "df_dup_k_c_val = dfcopy[m_dup_keys & ~m_c_inval]\n",
    "df_dup_k_c_inval= dfcopy[m_dup_keys & m_c_inval]\n",
    "print(f\"Nb of duplicated rows on key_cols with VALID code: {df_dup_k_c_val.shape[0]}\" )\n",
    "print(f\"Nb of duplicated rows on key_cols with INVALID code: {df_dup_k_c_inval.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WoEJ13qq8gkl"
   },
   "outputs": [],
   "source": [
    "# Duplicates on key_cols column with invalid codes\n",
    "# -> Keeping the combined row, deleting the others\n",
    "print(\"Shape of df_dup_k_c_inval, before\", df_dup_k_c_inval.shape[0])\n",
    "nb_gr, ind_drop, df_dup_f = comb_dup_rows(df_dup_k_c_inval, key_cols)\n",
    "df_dup_k_c_inval.drop(ind_drop, inplace=True)\n",
    "df_dup_k_c_inval.update(df_dup_f, overwrite=True)\n",
    "print(\"Nb of groups of duplicates :\", nb_gr)\n",
    "print(\"Nb of duplicates :\", df_dup_k_c_inval.shape[0])\n",
    "print(\"Nb of rows to delete:\", len(ind_drop))\n",
    "print(\"Shape of df_dup_k_c_inval, after\", df_dup_k_c_inval.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H46GGNHg3sNu"
   },
   "outputs": [],
   "source": [
    "# Changing dfcopy\n",
    "dfcopy.drop(ind_drop, inplace=True)\n",
    "dfcopy.update(df_dup_k_c_inval, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-GPDZshZvpLM"
   },
   "outputs": [],
   "source": [
    "# list(df.columns[df.columns.str.contains('nutri')]) 'nutriscore_score', 'nutrition-score-fr_100g'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D2YIf8mt4TjH"
   },
   "source": [
    "## 2 Cleaning numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "abMTPK_L4ST-"
   },
   "outputs": [],
   "source": [
    "dfcopy.select_dtypes(include=[float]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZF0dSplZNpB6"
   },
   "source": [
    "### 2.1 Dealing with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sdPRvk54i1LR"
   },
   "outputs": [],
   "source": [
    "cols_100g = list(dfcopy.columns[dfcopy.columns.str.contains('_100g')])\n",
    "df_100g = dfcopy.loc[:,cols_100g].astype(float).copy()\n",
    "df_cat_nutri = pd.concat([dfcopy.loc[:,'nutriscore'].astype(float), dfcopy.loc[:,['nutrigrade','pnns_gp1', 'main_categ_en']]]).copy()\n",
    "df_cat_nutri.columns=['nutriscore','nutrigrade','pnns_gp1', 'main_categ_en']\n",
    "df_cat_nutri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1NmKKB2b3xd3"
   },
   "outputs": [],
   "source": [
    "dfcopy.loc[:,'nutriscore'].isna().sum(), df_cat_nutri.loc[:,'nutriscore'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EM3bIQ-eFHsC"
   },
   "source": [
    "#### 2.1.1 columns about nutriscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tOD1Uh-Ax5GG"
   },
   "outputs": [],
   "source": [
    "speak(\"attention savastoppé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f4XkmEKMisKp"
   },
   "outputs": [],
   "source": [
    "# Checking if ther are rows where 'nutriscore' is different from 'nutriscore-fr'\n",
    "cond = dfcopy['nutriscore']==dfcopy['nutriscore-fr']\n",
    "dfcopy.dropna(subset=['nutriscore', 'nutriscore-fr'])[~cond].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OtyOMHqs2C0Z"
   },
   "source": [
    "Column 'nutriscore' is not different from 'nutriscore-fr'.\n",
    " We merge the columns and delete 'nutriscore-fr'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-tZKf7H7KP48"
   },
   "outputs": [],
   "source": [
    "dfcopy['nutriscore'] = dfcopy[\"nutriscore-fr\"].combine_first(dfcopy[\"nutriscore\"])\n",
    "dfcopy.drop(columns=[\"nutriscore-fr\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EwwaO1IXYcrz"
   },
   "source": [
    "The nutrigrade is assigned to a product after consideration of :\n",
    "- its nutriscore\n",
    "- whether it's solid food or beverage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QK2znlCK3HQK"
   },
   "outputs": [],
   "source": [
    "dfcopy.groupby('quantity_type').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YTki7BH_AS6J"
   },
   "outputs": [],
   "source": [
    "# dfcopy.boxplot(by=['nutrigrade', 'quantity_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l7KdtT2eAa4R"
   },
   "outputs": [],
   "source": [
    "gb = dfcopy.groupby(by=['nutrigrade', 'quantity_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-fhkaLnZdZKB"
   },
   "source": [
    "\"Outil_Calcul_Nutri-Score_SpF-030120\" :\n",
    "https://www.santepubliquefrance.fr/determinants-de-sante/nutrition-et-activite-physique/articles/nutri-score (tableur excel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iaTjyFScZk-I"
   },
   "source": [
    "1823 products are not identified as liquid/beverage or solid/food.\n",
    "Let's see if we can guess from the categories columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K3pMCSaRi1R8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A69gQcthE51a"
   },
   "source": [
    "#### 2.2.2 Columns about energy\n",
    "\n",
    "en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "186Figf2i-fI"
   },
   "outputs": [],
   "source": [
    "list(df_100g.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "34h182Fh7bxA"
   },
   "outputs": [],
   "source": [
    "df_100g.boxplot(['energy_100g', 'energy-from-fat_100g',\n",
    "                 'energy-kj_100g', 'energy-kcal_100g'],\n",
    "                vert=False, figsize=(21,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pLhW_na5TTN3"
   },
   "outputs": [],
   "source": [
    "# Distinguish situation 3 columns filled ?\n",
    "gb = df_100g.notna().groupby(by=['energy_100g', 'energy-kcal_100g', 'energy-kj_100g'])\n",
    "gb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SWCVkbYEgeNI"
   },
   "outputs": [],
   "source": [
    "# group1: all nan, can't do anything for the moment\n",
    "df_gp1 = df_100g.loc[gb.groups.get((False, False, False))]\n",
    "df_gp1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3xA-54nKZfbc"
   },
   "outputs": [],
   "source": [
    "# group2: group to send to the model -> is it kj or kcal ?\n",
    "df_gp2 = df_100g.loc[gb.groups.get((True, False, False))]\n",
    "df_gp2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3zzS6acKZdn4"
   },
   "outputs": [],
   "source": [
    "# group3: group where e is j\n",
    "df_gp3 = df_100g.loc[gb.groups.get((True, False, True))]\n",
    "print(df_gp3.shape)\n",
    "m_id3 = df_gp3.apply(lambda x: x['energy-kj_100g']==x['energy_100g'], axis=1)\n",
    "df_gp3[~m_id3].shape # cas où ils sont différents (2) : prévoir remplacement (fonction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pj5ZdCy6ocKO"
   },
   "outputs": [],
   "source": [
    "# group4: group where e is c \n",
    "df_gp4 = df_100g.loc[gb.groups.get((True, True, False))]\n",
    "print(df_gp4.shape)\n",
    "m_id4 = df_gp4.apply(lambda x: x['energy-kcal_100g']==x['energy_100g'], axis=1)\n",
    "df_gp4[~m_id4].shape # cas où ils sont différents (0) : prévoir remplacement (fonction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vyhSsZbWoh_2"
   },
   "outputs": [],
   "source": [
    "# group5: group where e, c and j are both filled : are they the same ? which is the correct one ? to be submitted to the model\n",
    "df_gp5 = df_100g.loc[gb.groups.get((True, True, True))]\n",
    "print(df_gp5.shape)\n",
    "f_e_eq_c = lambda x: x['energy-kcal_100g']==x['energy_100g']\n",
    "f_e_eq_j = lambda x: x['energy-kj_100g']==x['energy_100g']\n",
    "m_id_c5 = df_gp5.apply(f_e_eq_c, axis=1)\n",
    "m_id_j5 = df_gp5.apply(f_e_eq_j, axis=1)\n",
    "df_gp5_A = df_gp5[(m_id_c5&m_id_j5)]\n",
    "df_gp5_B = df_gp5[(~m_id_c5)&(~m_id_j5)]\n",
    "df_gp5_C = df_gp5[(~(m_id_c5&m_id_j5))&(~((~m_id_c5)&(~m_id_j5)))]\n",
    "df_gp5_C_c = df_gp5_C[m_id_c5]\n",
    "df_gp5_C_j = df_gp5_C[m_id_j5]\n",
    "print(df_gp5_A.shape) # cas où j, c et e sont égaux (env 2 000)\n",
    "print(df_gp5_B.shape) # cas où e, c et j sont tous différents (0)\n",
    "print(df_gp5_C.shape) # cas où j, e et c ne sont pas tous égaux mais pas tous différents (au moins un couple égaux) (env 20 000)\n",
    "print(df_gp5_C_c.shape)\n",
    "print(df_gp5_C_j.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VF6IBSWYoiPN"
   },
   "outputs": [],
   "source": [
    "'''données False True True inexistantes \n",
    "préparer un modèle pour détecter la catégorie de calorie (True False False)\n",
    "éliminer données False False False\n",
    "d'abord modifier les données de type True True True en : True True False ou True, False true\n",
    "puis garder les données de type\n",
    "True, False, True ou\n",
    "True, True, False\n",
    "------------------\n",
    "les données auront 4 colonnes et 1 target\n",
    "target à créer : kcal 0 ou 1\n",
    "mettre en place un modèle random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZjlLjwD3oiaM"
   },
   "outputs": [],
   "source": [
    "# Gathering rows and columns to forme a dataset\n",
    "sel_en_cols = ['energy_100g','fat_100g', 'carbohydrates_100g', 'proteins_100g',\n",
    "            'fiber_100g', 'alcohol_100g', 'pnns_gp1', 'main_categ_en', 'nutriscore', 'nutrigrade']\n",
    "train_set = pd.concat([df_gp3[m_id3], df_gp3[m_id3]])[sel_en_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "84bG-eDbz2HT"
   },
   "outputs": [],
   "source": [
    "train_set.drop(columns=['energy_calc_kcal_100g', 'energy_calc_kj_100g'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "od3p4-fv1Utb"
   },
   "outputs": [],
   "source": [
    "train_set[['fat_100g', 'proteins_100g','carbohydrates_100g']].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HEWy240OzACj"
   },
   "outputs": [],
   "source": [
    "# Creating two calculated versions of energy (kcal) and (kj)\n",
    "def calc_en_from_c_f_p(df):\n",
    "  res_c = df['fat_100g'].fillna(0)*9\\\n",
    "          +(df['carbohydrates_100g'].fillna(0)+df['proteins_100g'.fillna(0)])*4\\\n",
    "          +df['fiber_100g'].fillna(0)*1.9+df['alcohol_100g'].fillna(0)*7\n",
    "  res_j = res_c*418.4\n",
    "  return res_c, res_j\n",
    "\n",
    "ser_c,ser_j = calc_en_from_c_f_p(train_set)\n",
    "# train_set.insert(1, 'energy_calc_kcal_100g', ser_c)\n",
    "# train_set.insert(2, 'energy_calc_kj_100g', ser_j)\n",
    "# train_set.sample(5)\n",
    "\n",
    "calc_en_from_c_f_p(train_set)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PZ0QmH2OYNz1"
   },
   "outputs": [],
   "source": [
    "df_100g.boxplot(column=['energy_100g'], by='cal_j',vert=False, figsize=(21,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZtJaAi4qQjz0"
   },
   "outputs": [],
   "source": [
    "# @interact\n",
    "# def adjust_a(a=(0,1,0.1)):\n",
    "#   return [a*x for x in range(0,35000)]\n",
    "  \n",
    "# ax0 = df_100g.plot(kind='scatter', x='energy-kj_100g', y='energy-kcal_100g', c='red', s=5)\n",
    "# df_100g.plot(kind='line', x='energy-kj_100g', y=adjust_a(), c='grey', ax=ax0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R5myWfXPiruv"
   },
   "outputs": [],
   "source": [
    "ax1 = df_100g.plot(kind='scatter', x='energy_100g', y='energy-kcal_100g', c='red', s=50)\n",
    "for col,c,s in [('energy-kj_100g','b',25), ('energy_100g','grey',1)]:\n",
    "  df_100g.plot.scatter(x='energy_100g', y=col, s=s, c=c,ax=ax1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s5Spn-6c6i5b"
   },
   "outputs": [],
   "source": [
    "speak(\"Abracadabraaaaa é voilà \")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMY0VwQWSQu2cG+xQ73slk8",
   "collapsed_sections": [],
   "name": "P3_Cleaning_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
