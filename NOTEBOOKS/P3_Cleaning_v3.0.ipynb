{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"P3_Cleaning_v3.0.ipynb","provenance":[{"file_id":"1CjFqLqI3e83aWkErpy2_tDoqWWSc5V7K","timestamp":1567509524556},{"file_id":"1oFtNqY9sTtyX09HnsLY5GOpchE7TWSLc","timestamp":1567440734485},{"file_id":"1kO3qnFJ8XAhA2WzueAy6Gwr0KwfoSF2z","timestamp":1566893631574},{"file_id":"1rI7P6dn7-IGK6p8HX7dvNP93roGSzXeX","timestamp":1566833630097},{"file_id":"1cAbXwtjxfOIVmnecCFlHrVryskoYJCXA","timestamp":1566734390006},{"file_id":"https://github.com/SmellyArmure/PROJECT3/blob/master/NOTEBOOKS/P3_Cleaning_v1_0.ipynb","timestamp":1566726579304}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"}},"cells":[{"cell_type":"markdown","metadata":{"id":"pVbrKjWrGl4m","colab_type":"text"},"source":["# \"Concevez une application au service de la santé publique\"\n","_Cleaning Notebook_\n","13/08 16h"]},{"cell_type":"markdown","metadata":{"id":"NQ8_ZaJvGl4o","colab_type":"text"},"source":["### Importing Modules"]},{"cell_type":"code","metadata":{"id":"AWVZdsRnGl4p","colab_type":"code","outputId":"f10eb7c3-0e9a-4c3a-cd29-fb693e29ba6d","executionInfo":{"status":"ok","timestamp":1567509590499,"user_tz":-120,"elapsed":1989,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["## IMPORTATIONS\n","import pandas as pd\n","import pandas_profiling\n","pd.set_option('display.max_rows', 500)\n","pd.set_option('display.max_columns', 500)\n","pd.set_option('display.width', 1000)\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","% matplotlib inline\n","import seaborn as sns\n","sns.set_style(\"whitegrid\")\n","\n","import scipy.stats as st\n","\n","import re\n","\n","from google.colab import files\n","\n","import sys\n","sys.executable # 'C:\\\\ProgramData\\\\Anaconda3\\\\python.exe'"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/usr/bin/python3'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"k0z6zmMiGl4t","colab_type":"code","colab":{}},"source":["# %pip install jupyternotify\n","# import jupyternotify\n","# ip = get_ipython()\n","# ip.register_magics(jupyternotify.JupyterNotifyMagics)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0H4oAGV-Gl41","colab_type":"text"},"source":["### Defining functions"]},{"cell_type":"code","metadata":{"id":"DkVlt1AubGlw","colab_type":"code","colab":{}},"source":["def Diff(tab1, tab2): \n","    #tab_dif = [i for i in tab1 + tab2 if i not in tab1 or i not in tab2]\n","    return (set(tab1)-set(tab2),set(tab2)-set(tab1))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E4sTt5SUGl42","colab_type":"code","colab":{}},"source":["def info (df):\n","    nb_li = df.shape[0]\n","    nb_co = df.shape[1]\n","    t = np.empty(nb_li)\n","    t.fill(nb_li)\n","    df_l_null = pd.DataFrame(df.T.isna().sum()) # number of null per row\n","    df_c_null = pd.DataFrame(df.isna().sum()) # number of null per column\n","    \n","    # number of rows without any 'null'\n","    al = len([x for x in df_l_null[0] if x==0])\n","    nb_ss_null = pd.DataFrame([al]).rename(index={0:'row_wo_null'}).T\n","    pct_ss_null = pd.DataFrame([al*100/nb_li]).rename(index={0:'row_wo_null'}).T\n","    # number of rows with all 'null'\n","    bl = len([x for x in df_l_null[0] if x==nb_co])\n","    nb_null = pd.DataFrame([bl]).rename(index={0:'row_null'}).T\n","    pct_null = pd.DataFrame([bl*100/nb_li]).rename(index={0:'row_null'}).T\n","    # number of mixed rows (with some null)\n","    cl = len([x for x in df_l_null[0] if (x!=0 and x!=nb_co)])\n","    nb_mix = pd.DataFrame([cl]).rename(index={0:'row_mix'}).T\n","    pct_mix = pd.DataFrame([cl*100/nb_li]).rename(index={0:'row_mix'}).T\n","    info_nb = pd.concat([nb_ss_null, nb_null,\n","                         nb_mix],axis=1, sort=False).rename(index={0:'nb'})\n","    info_pct = pd.concat([pct_ss_null, pct_null,\n","                          pct_mix],axis=1, sort=False).rename(index={0:'pct'})\n","    info_l = pd.concat([info_nb,info_pct], sort=False)\n","    # total number of rows\n","    info_l[\"row_tot\"] = [info_l.T['nb'].sum(), info_l.T['pct'].sum()]\n","    \n","    # number of columns without any 'null''\n","    ac = len([x for x in df_c_null[0] if x==0])\n","    nb_ss_null = pd.DataFrame([ac]).rename(index={0:'col_wo_null'}).T\n","    pct_ss_null = pd.DataFrame([ac*100/nb_co]).rename(index={0:'col_wo_null'}).T\n","    # number of columns with all 'null'\n","    bc = len([x for x in df_c_null[0] if x==nb_li])\n","    nb_null = pd.DataFrame([bc]).rename(index={0:'col_null'}).T\n","    pct_null = pd.DataFrame([bc*100/nb_co]).rename(index={0:'col_null'}).T\n","    # number of mixed columns (with some null)\n","    cc = len([x for x in df_c_null[0] if (x!=0 and x!=nb_li)])\n","    nb_mix = pd.DataFrame([cc]).rename(index={0:'col_mix'}).T\n","    pct_mix = pd.DataFrame([cc*100/nb_co]).rename(index={0:'col_mix'}).T\n","    info_nb = pd.concat([nb_ss_null, nb_null,\n","                         nb_mix],axis=1, sort=False).rename(index={0:'nb'})\n","    info_pct = pd.concat([pct_ss_null, pct_null, pct_mix],\n","                         axis=1, sort=False).rename(index={0:'pct'})\n","    info_c = pd.concat([info_nb,info_pct], sort=False)\n","    # total number of columns\n","    info_c[\"col_tot\"] = [info_c.T['nb'].sum(), info_c.T['pct'].sum()]\n","    \n","    info = pd.concat([info_l,info_c], axis=1, sort=False)\n","    \n","    return info"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7_5C_t3sGl46","colab_type":"code","colab":{}},"source":["def desc_bis (df):\n","    nb_li = df.index.size\n","    nb_col = df.columns.size\n","    tot = nb_li*nb_col    \n","    info = pd.DataFrame(df.dtypes).T.rename(index={0:'type'}) \n","    info = info.append(pd.DataFrame(df.isna().sum()).T.rename(index={0:'null'}))\n","    return info"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OPQOig7nGl49","colab_type":"code","colab":{}},"source":["def eval_null (inf_df):\n","    a = inf_df.T['null'].sum()\n","    b = inf_df.T['count'].sum()\n","    print(\"Number of 'null' values: {:.0f}\".format(a))\n","    print(\"Number of values (without 'null'): {:.0f}\".format(b))\n","    print(\"Total number of cells: {:.0f}\".format(a+b))\n","    print(\"Total number of 'null' (pct): {:.1f}%\".format(a*100/(a+b)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fmDu0rMFGl5A","colab_type":"text"},"source":["### Importing dataset"]},{"cell_type":"code","metadata":{"id":"7eIHbz0NGl5C","colab_type":"code","colab":{}},"source":["# On my PC: Import data from csv (part or all of the rows)\n","#filepath = '../DATA/en.openfoodfacts.org.products.csv'\n","#df = pd.read_csv(filepath, sep='\\t', low_memory=False, encoding ='utf-8')\n","#df = pd.read_csv(filepath, sep='\\t',nrows=500000, low_memory=False, encoding ='utf-8')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2NwAMfsWJD9m","colab_type":"code","colab":{}},"source":["# On Google Colab (1): Upload a file from my drive\n","# from google.colab import files\n","# import io\n","# uploaded = files.upload()\n","# df = pd.read_csv(io.BytesIO(df['en.openfoodfacts.org.products.csv']))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hWVOTyi6ILSB","colab_type":"code","outputId":"41afeb05-71c0-444f-a2b1-8607928d1e5d","executionInfo":{"status":"ok","timestamp":1567510039984,"user_tz":-120,"elapsed":451393,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["# On Google Colab (2): Upload a file from Google Drive via PyDrive\n","# Install PyDrive wrapper & import libraries (once)\n","!pip install -U -q PyDrive \n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","# Authenticate and create PyDrive client (once)\n","auth.authenticate_user() \n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","# Download a file based on its file ID (shareable link)\n","file_id = '1Ua1RzJPjLAaDg9kgz3SmusmGoUO53OOx'\n","downloaded = drive.CreateFile({'id': file_id})"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\u001b[?25l\r\u001b[K     |▎                               | 10kB 25.2MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 7.2MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 10.2MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 6.3MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 7.7MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 9.1MB/s eta 0:00:01\r\u001b[K     |██▎                             | 71kB 10.3MB/s eta 0:00:01\r\u001b[K     |██▋                             | 81kB 11.5MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 12.8MB/s eta 0:00:01\r\u001b[K     |███▎                            | 102kB 10.1MB/s eta 0:00:01\r\u001b[K     |███▋                            | 112kB 10.1MB/s eta 0:00:01\r\u001b[K     |████                            | 122kB 10.1MB/s eta 0:00:01\r\u001b[K     |████▎                           | 133kB 10.1MB/s eta 0:00:01\r\u001b[K     |████▋                           | 143kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 153kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 163kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 174kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 184kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 194kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 204kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 215kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 225kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 235kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 245kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 256kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 266kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 276kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 286kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 296kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 307kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 317kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 327kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 337kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 348kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 358kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 368kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 378kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 389kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 399kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 409kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 419kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 430kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 440kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 450kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 460kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 471kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 481kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 491kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 501kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 512kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 522kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 532kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 542kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 552kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 563kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 573kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 583kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 593kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 604kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 614kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 624kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 634kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 645kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 655kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 665kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 675kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 686kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 696kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 706kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 716kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 727kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 737kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 747kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 757kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 768kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 778kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 788kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 798kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 808kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 819kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 829kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 839kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 849kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 860kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 870kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 880kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 890kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 901kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 911kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 921kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 931kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 942kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 952kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 962kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 972kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 983kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 993kB 10.1MB/s \n","\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"L5iaFiqYGl5H","colab_type":"code","colab":{}},"source":["downloaded.GetContentFile('en.openfoodfacts.org.products.csv')  \n","df = pd.read_csv('en.openfoodfacts.org.products.csv',\n","                 sep='\\t', low_memory=False, encoding ='utf-8')  # nrows=1000,"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WJMsiWvOPV-S","colab_type":"code","colab":{}},"source":["# df.head(2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iwXHJlEdGl5K","colab_type":"text"},"source":["### Description"]},{"cell_type":"code","metadata":{"id":"cDUMqJudGl5L","colab_type":"code","colab":{}},"source":["# # Shape of the dataset\n","# df.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gN_ZGypmGl5N","colab_type":"code","colab":{}},"source":["# desc_df_all = desc_bis(df).append(df.describe(include='all'))\n","# desc_df_all"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p9_5IC9GGl5Q","colab_type":"code","colab":{}},"source":["# desc_df_num = df.describe(include=['int64', 'float64'])\n","# desc_df_num = desc_bis(df[desc_df_num.columns]).append(desc_df_num)\n","# desc_df_num"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3y6_o44LGl5Z","colab_type":"code","colab":{}},"source":["# dfcopy = df.copy(deep = True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"zBs1wumbGl5c","colab_type":"code","colab":{}},"source":["# On Google Colab\n","# profile = pandas_profiling.ProfileReport(dfcopy) #, check_correlation = False\n","\n","# On my PC\n","# profile = dfcopy.profile_report(dfcopy, check_correlation = False)\n","# profile = dfcopy.profile_report(title=\"OpenFoodFacts dataset\",\n","#                                 style={'full_width':True},\n","#                                 correlations={'pearson': True,\n","#                                               'spearman': False,\n","#                                               'kendall': False,     \n","#                                               'phi_k': False,\n","#                                               'cramers': False,\n","#                                               'recoded': False},\n","#                                 plot ={'histogram':{'bayesian_blocks_bins': False, 'bins': 12}},\n","#                                )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m3AgzWQcGl5f","colab_type":"code","colab":{}},"source":["# profile.to_file(\"OFFacts_report2_3.html\")\n","# profile"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VJ6tjq8lGl5j","colab_type":"code","colab":{}},"source":["# # extract rejected columns (based on correlation coeff greater than 0.9)\n","# rejected_variables_90 = profile.get_rejected_variables(threshold=0.9)\n","# len(rejected_variables_90)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TRvAQUwBGl5s","colab_type":"text"},"source":["### Data structure"]},{"cell_type":"markdown","metadata":{"id":"ZpbMgPvfGl5t","colab_type":"text"},"source":["NB: The \"data-fields.txt\" file mentions 143 columns whereas pd.describe reveals the actual base have 175."]},{"cell_type":"markdown","metadata":{"id":"zong8UIYGl5u","colab_type":"text"},"source":["### Missing values\n","#### Vizualisation"]},{"cell_type":"code","metadata":{"id":"WOEkCz5KGl5v","colab_type":"code","colab":{}},"source":["# # Number of 'null' rows and columns in the dataset\n","# info_df = info(df)\n","# pd.options.display.float_format = '{:.0f}'.format\n","# info_df.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EhIXr2MiGl54","colab_type":"code","colab":{}},"source":["# # Giving global evaluation of the number of 'null' values\n","# eval_null(desc_df_all)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WDNKuPJWLLfC","colab_type":"code","colab":{}},"source":["# # Vizualisation of missing values\n","# # columns vs. xx % null/non-null values\n","\n","# nb_col_non_null = desc_df_all.loc[\"count\"].values.astype(int) # nb of null for each col\n","# pct_col_non_null = nb_col_non_null*100/df.shape[0] # pct of null for each col\n","\n","# nb_row_non_null = df.count(axis=1).values.astype(int) # nb of null for each row\n","# pct_row_non_null = nb_row_non_null*100/df.shape[1] # pct of null for each row\n","\n","# fig1 = plt.figure(figsize = (13,6))\n","\n","# ax1 = plt.subplot(2,2,1)\n","# ax1.hist(pct_row_non_null, bins=20, density=False, alpha =0.5,\n","#          histtype='stepfilled', cumulative=False,\n","#          fc='blue', ec='k', label='20 bins')\n","# ax1.hist(pct_row_non_null, bins=500, density=False,\n","#          histtype='step', cumulative=False,\n","#          fc='blue', ec='k', label='500 bins')\n","# plt.xlabel(\"Percent of null values\", fontsize=14)\n","# plt.ylabel(\"Number of rows\", fontsize=14)\n","# #plt.xlim(40, 160), plt.ylim(0, 0.03)\n","# #plt.text(0,my_y_lim*0.05, \"blabla\", fontsize=14)\n","# plt.title(\"Histogram (nb of rows vs. pct of null)\", \n","#           fontsize=14, fontweight='bold')\n","# plt.grid(True), plt.legend(loc='upper right', fontsize=12)\n","\n","# ax2 = plt.subplot(2,2,2)\n","# ax2.hist(pct_row_non_null, bins=500, fc='b', alpha =0.5, ec='b', \n","#          density=False, histtype='stepfilled', cumulative=True,\n","#          label='500 bins')\n","# ax2.hist(pct_row_non_null, bins = 20, fc = 'None', ec = 'k', \n","#          density=False, histtype='step', cumulative=True, \n","#          label='20 bins')\n","# plt.xlabel(\"Percent of null values\", fontsize=14)\n","# plt.ylabel(\"Number of rows\", fontsize=14)\n","# #plt.xlim(40, 160), plt.ylim(0, 0.03)\n","# #plt.text(0,my_y_lim*0.05, \"blabla\", fontsize=14)\n","# plt.title(\"Cumulative histogram (nb of rows vs. pct of null)\",\n","#           fontsize=14, fontweight='bold')\n","# plt.grid(True), plt.legend(loc='upper left',fontsize=12)\n","\n","# # Vizualisation of existing values\n","# # columns with xx % non-null value\n","\n","# nb_non_null = desc_df_all.loc[\"count\"].values.astype(int) # nb of null for each col\n","# pct_non_null = nb_non_null*100/df.shape[0] # pct of null for each col\n","\n","# ax1 = plt.subplot(2,2,3)\n","# ax1.hist(pct_col_non_null, bins=20, density=False, alpha =0.5,\n","#          histtype='stepfilled', cumulative=False,\n","#          fc='blue', ec='k', label='20 bins')\n","# ax1.hist(pct_col_non_null, bins=500, density=False,\n","#          histtype='step', cumulative=False,\n","#          fc='blue', ec='k', label='500 bins')\n","# plt.xlabel(\"Percent of non-null values\", fontsize=14)\n","# plt.ylabel(\"Number of columns\", fontsize=14)\n","# #plt.xlim(40, 160), plt.ylim(0, 0.03)\n","# #plt.text(0,my_y_lim*0.05, \"blabla\", fontsize=14)\n","# plt.title(\"Histogram (nb of columns vs. pct of non-null)\", \n","#           fontsize=14, fontweight='bold')\n","# plt.grid(True), plt.legend(fontsize=12)\n","\n","# ax2 = plt.subplot(2,2,4)\n","# ax2.hist(pct_col_non_null, bins=500, fc='b', alpha =0.5, ec='b', \n","#          density=False, histtype='stepfilled', cumulative=True,\n","#          label='500 bins')\n","# ax2.hist(pct_col_non_null, bins = 20, fc = 'None', ec = 'k', \n","#          density=False, histtype='step', cumulative=True, \n","#          label='20 bins')\n","# plt.xlabel(\"Percent of non-null values\", fontsize=14)\n","# plt.ylabel(\"Number of columns\", fontsize=14)\n","# #plt.xlim(40, 160), plt.ylim(0, 0.03)\n","# #plt.text(0,my_y_lim*0.05, \"blabla\", fontsize=14)\n","# plt.title(\"Cumulative histogram (nb of columns vs. pct of non-null)\",\n","#           fontsize=14, fontweight='bold')\n","# plt.grid(True), plt.legend(loc='upper left',fontsize=12)\n","# fig1.tight_layout()\n","\n","# plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A63XUKmFTOPs","colab_type":"code","colab":{}},"source":["# # Producing table with number and pct of missing/non-missing values for each column\n","# df_null = desc_df_all.loc[['null','count']].T.sort_values(by = 'count')\n","# df_null[\"pct_null\"] = df_null[\"null\"]*100/df.shape[0]\n","# df_null[\"pct_non_null\"] = df_null[\"count\"]*100/df.shape[0]\n","# #df_null[\"total\"] = df_null[\"count\"]+df_null[\"null\"]\n","# df_null = df_null.sort_values(by = 'count')\n","# pd.options.display.float_format = '{:.2f}'.format\n","# df_null.head(7)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B-2qidoKGl6C","colab_type":"text"},"source":["### Dealing with duplicates\n","Step by step :\n","- _absolute duplicates_ : keep the first\n","- _duplicates among key columns ('code')_ :\n","keep the row with less missing values, and if there're the same, keep the row with most distinct values\n","\n","(One Function including all the steps \"dealing_duplicates\")"]},{"cell_type":"code","metadata":{"id":"hQSsPQKNjVJL","colab_type":"code","colab":{}},"source":["df_clean = df.copy(deep = True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JKh1VAZeRFzC","colab_type":"code","colab":{}},"source":["# Check nb of duplicated rows\n","\n","def check_duplicates(df, key_cols) : \n","  dup_key_df = df_clean[df_clean.duplicated(subset=key_cols,\n","                                            keep=False).values]\n","  dup_key_df = dup_key_df.fillna('missing').loc[:,key_cols]\\\n","                         .reset_index().groupby(by=key_cols).count()\n","  # NB : necessary to use fillna because groupby doesn't consider nan as a value\n","  return  dup_key_df.shape[0], dup_key_df.sum().values[0]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pLnGST-EHhEv","colab_type":"code","outputId":"d41ef021-968f-4908-f1d1-ef63560d0774","executionInfo":{"status":"ok","timestamp":1567512586579,"user_tz":-120,"elapsed":2056,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["key_cols = ['code','product_name','quantity','brands']\n","#key_cols = list(df.columns)\n","dup_t = check_duplicates(df, key_cols)\n","print(\"nb of groups of duplicates : \", dup_t)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(117, 234)"]},"metadata":{"tags":[]},"execution_count":86}]},{"cell_type":"code","metadata":{"id":"V2Cpbo7ySJ3L","colab_type":"code","colab":{}},"source":["def dealing_duplicates (df, key_cols):\n","\n","  df_clean = df.copy(deep = True)\n","\n","  # Delete 'null' rows, if existing\n","  df_clean.dropna(how = 'all', axis = 0, inplace = True)\n","\n","  # Delete duplicated rows (all columns)\n","  dup_all_ser = df_clean.duplicated(subset=None, keep='first')\n","  df_clean.drop(index = df_clean[dup_all_ser.values == True].index, inplace=True)\n","\n","  # Dealing with duplicated rows (among key columns)\n","  dup_key_df = df_clean[df_clean.duplicated(subset=key_cols,\n","                                            keep=False).values]\n","\n","  # ranging indexes by decreasing amount of 'nan's then by distinct values\n","  li_ind = pd.DataFrame({'n_rens' : df.isna().T.sum().values,\n","                'n_dist' : len(df.iloc[1].T.unique())},\n","                              index = ech.index.values)\\\n","          .sort_values(by=[\"n_rens\", \"n_dist\"], ascending = True).index\n","  df_comb = df.loc[li_ind[0]:li_ind[0]]\n","  # combine the rows in appropriate order\n","  for ind in li_ind:\n","      df_comb = df_comb.reset_index().combine_first(df.loc[ind:ind]\\\n","                       .reset_index()).set_index('index').rename_axis(None)\n","  # replaces the unique row and drop useless rows\n","  df_clean.update(df_comb, overwrite=True)\n","  df_clean.drop(li_ind[1:], inplace = True)\n","\n","  return df_clean\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w86q6YUYKUW-","colab_type":"code","colab":{}},"source":["df_cleanbis = dealing_duplicates (df, ['code'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4gua0xUabj1I","colab_type":"code","outputId":"ff9f085d-907f-4b85-b73f-361d5674f424","executionInfo":{"status":"ok","timestamp":1567510242445,"user_tz":-120,"elapsed":997,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["df_cleanbis.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(954302, 175)"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"id":"l528FKziOEOy","colab_type":"code","colab":{}},"source":["# 2 éliminer les entrées qui n'ont pas de code, ou pas de nom de produit"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gilyAtmf9N5s","colab_type":"code","colab":{}},"source":["#np.nansum(dup_key_df.groupby(by=['code','product_name','quantity','brands']).count()[\"url\"].values)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WG3rzfxcFpFP","colab_type":"code","colab":{}},"source":["# New column 'quantity_norm' with normalized quantities (put all in g, numeric)\n","#df_clean[\"quantity\"]\n","#df.replace(to_replace=r'g', value='', regex=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cL-2OYjkLiZz","colab_type":"code","colab":{}},"source":["#dup_key_df.groupby(by=['code','product_name','quantity','brands']).count().shape#"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DmcHrPizHKYZ","colab_type":"code","colab":{}},"source":["# dup_code_ser = df.duplicated(subset=['code'], keep='first')\n","# dup_name_ser = df.duplicated(subset=['product_name'], keep='first')\n","# dup_code_name_ser = df.duplicated(subset=['code','product_name'], keep='first')\n","# dup_name_quant_ser = df.duplicated(subset=['quantity','product_name'], keep='first')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ozSv9UzkOKH3","colab_type":"code","colab":{}},"source":["# Rows with erroneous barcode\n","# number of digits\n","# code_len_tab = df[\"code\"].apply(len).values\n","# nb_dig_code_df = pd.DataFrame(np.histogram(code_len_tab, bins = max(code_len_tab)))\n","# nb_dig_code_df.\n","\n","# key control"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UXWZkBlcUZ71","colab_type":"code","colab":{}},"source":["# # Calculating the EAN check digit for 13-digit numbers\n","# def calc_check_digit(number):\n","#     first = \n","#     last = \n","#     if len(number) not in (14, 13, 12, 8):\n","#         return False\n","#     else:\n","#         return str((10 - sum((3, 1)[i % 2] * int(n)\n","#                          for i, n in enumerate(reversed(number)))) % 10)\n","        \n","# def validate(number):\n","#     \"\"\"Check if the number provided is a valid EAN-13. This checks the length\n","#     and the check bit but does not check whether a known GS1 Prefix and\n","#     company identifier are referenced.\"\"\"\n","#     number = compact(number)\n","#     if not isdigits(number):\n","#         raise InvalidFormat()\n","#     if len(number) not in (14, 13, 12, 8):\n","#         raise InvalidLength()\n","#     if calc_check_digit(number[:-1]) != number[-1]:\n","#         raise InvalidChecksum()\n","#     return number"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DUUE6taUUaYM","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"idXUcsIHF3Kr","colab_type":"text"},"source":["### Reconstructing/fixing the columns"]},{"cell_type":"code","metadata":{"id":"-6NGa7Tidc6I","colab_type":"code","colab":{}},"source":["# Checking the number and extracting names of 'null' columns, if existing\n","# col_dropna_df = df.dropna(how = 'all', axis = 1, inplace = False)\n","# cols_null = [col for col in df.columns if col not in col_dropna_df.columns]\n","# len(cols_null), cols_null"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KOhcqLsCLLjC","colab_type":"code","colab":{}},"source":["# Number of duplicates considering all columns or selection of crucial columns\n","# dup_all_ser.sum(), dup_code_ser.sum(), dup_name_ser.sum(),\\\n","# dup_code_name_ser.sum(), dup_name_quant_ser.sum()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nc5GvGXmsGSy","colab_type":"code","colab":{}},"source":["# Rows with erroneous name ('' or numbers)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lK_5W_GBXiKB","colab_type":"code","colab":{}},"source":["# Checking number and extracting names of columns with more than 2.5% missing values ('null')\n","# cols_miss_val = df_null[df_null['pct_non_null']<2.5].index\n","# len(cols_miss_val), cols_miss_val"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-kfYPHcfRDJW","colab_type":"code","colab":{}},"source":["# Diff(rejected_variables_90,cols_miss_val)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RBu-szZiRljG","colab_type":"text"},"source":["__Main conclusions__ :\n"," - 14 columns with no data\n"," - 78 columns with very few data (less than 2.5 %)\n"," - 0 row with no data\n"," -  rows with crucial data missing"]},{"cell_type":"markdown","metadata":{"id":"6cB4AQ5CGl6B","colab_type":"text"},"source":["#### Dealing with missing values"]},{"cell_type":"code","metadata":{"id":"n6lGPs0FdJTx","colab_type":"code","colab":{}},"source":["# Eliminating rows with crucial data missing\n","# (no null row to eliminate)\n","# df_clean = df_clean.drop(columns = col_miss_val)\n","# df_clean.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"coHLGJNmJrL0","colab_type":"code","colab":{}},"source":["# Eliminating columns with more than 2.5% missing values\n","# (no null row to eliminate)\n","# df_clean = df_clean.drop(columns = col_miss_val)\n","# df_clean.shape"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6bKZ5sDfdH_q","colab_type":"text"},"source":["fdcxwsfcxfgvxc"]},{"cell_type":"markdown","metadata":{"id":"uOHPT87KGl6D","colab_type":"text"},"source":["#### -> dealing with duplicates"]},{"cell_type":"markdown","metadata":{"id":"Vo9mHiCPGl6E","colab_type":"text"},"source":["\n","- retirer les produits sans noms\n","- garder seulement les items ayant plus de XX % de données renseignées (éliminer d'abord les lignes)\n","\n","- respect des bornes (0-100g)\n","    - combien d'entrées concernés\n","    - facile à corriger ou pas ?\n","\n","- outliers exceeding 3 sigmas\n","    - nombreux ou pas ?\n","    - traitement au cas par cas éventuellement\n","\n","- trouver la structure des données/catégorisation des colonnes\n","- élimination ou sélection de certaines colonnes ?\n","- vérifier la cohérence des infos (calories/joules et somme des énergies des différentes classes)\n","\n","- reconstruire les valeurs manquantes\n","\n","Exploration\n","Analyse univariée :\n","- box plot, violin plot, nuages de dispersion\n","- hisogrammes pour différentes variables\n","- produits ayant le plus grand nombres d'ingrédients\n","\n","Analyse mulivariée :\n","- matrice de corrélation pour plusieurs variables\n","- diagramme de dispersion en fonction du nutriscore pour pls groupes de 2 variables"]},{"cell_type":"markdown","metadata":{"id":"ul0J47VeGl6E","colab_type":"text"},"source":["### Outliers"]},{"cell_type":"markdown","metadata":{"id":"elrYgdBIGl6F","colab_type":"text"},"source":["#### -> dealing with outliers"]},{"cell_type":"markdown","metadata":{"id":"VJhhNaiEGl6G","colab_type":"text"},"source":["### Rescaling data"]},{"cell_type":"code","metadata":{"id":"f1T6InqiGl6J","colab_type":"code","colab":{}},"source":["# separating numerical data from object data\n","# num_df = df.select_dtypes(include=\"number\")\n","# obj_df = df.select_dtypes(include=\"object\")\n","# df.shape, num_df.shape, obj_df.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_pGO2Mw3Gl6L","colab_type":"code","colab":{}},"source":["# # no need for the datetime columns\n","# # cols = [c for c in df.columns if c.lower()[-9:] != '_datetime' and c.lower()[-2:] != '_t']\n","# cols = [c for c in df.columns if c.lower()[-5:] == '_100g']\n","# cols.append('code')\n","# df = df[cols]\n","\n","# # eliminate null values\n","# df = df[(df['energy_100g'] != 0) & (df['nutrition-score-fr_100g'] != 0)]\n","\n","# # we keep only column 70% filled\n","# thresh = len(df) * .3\n","# df.dropna(thresh = thresh, axis = 1, inplace = True)\n","\n","# df = df.fillna(0)\n","\n","# df = df.round(5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GT9tc69-Gl6O","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mxSJdUfEGl6R","colab_type":"text"},"source":["### Define categories"]},{"cell_type":"code","metadata":{"id":"oRASxl7qGl6U","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zAYBPhYZGl6W","colab_type":"code","colab":{}},"source":["# posteDep = {\n","#     'EMPLOI': 'Entrées',\n","#     'PAIE': 'Entrées',\n","#     'SAADNA': 'E"],"execution_count":0,"outputs":[]}]}