{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting list of countries\n",
    "# files.upload()\n",
    "f=open('countries.txt', \"r\")\n",
    "content = f.read()\n",
    "f.close()\n",
    "list_cnt = content.lower().split(',')\n",
    "list_cnt = [s.strip() for s in list_cnt]\n",
    "\n",
    "# Cleaning strings in 'countries_en'\n",
    "df[['countries_en', 'countries']] = df[['countries_en', 'countries']].astype('object')\n",
    "df[['countries_en', 'countries']].fillna('unknown')\n",
    "df['countries_en'] = df['countries_en'].str.lower().apply(str)\n",
    "df['countries_en'] = df['countries_en'].str.replace('ô', 'o').str.replace('é', 'e')\n",
    "\n",
    "# Creating a new column with filtered list of countries\n",
    "split_remove = lambda x: tuple([s for s in x.split(',') if s in list_cnt])\n",
    "df['countries_en_tuple'] = df['countries_en'].apply(split_remove)\n",
    "# Creating a column with only one main (first) country\n",
    "df['countries_en_first'] = df['countries_en_tuple']\\\n",
    "                          .apply(lambda x: x[0] if len(x)!=0 else 'unknown')\n",
    "# Reordering the columns\n",
    "df = df.reindex(columns=move_cat_containing(df.columns,['countr'], 'first'))\n",
    "\n",
    "# Checking new columns\n",
    "cols = ['countries', 'countries_en', 'countries_en_first', 'countries_en_tuple']\n",
    "df[cols].sample(5)\n",
    "\n",
    "# Deleting old columns\n",
    "del df['countries'], df['countries_en']\n",
    "\n",
    "df['countries_en_first'].value_counts()[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data, dropping categories with less than 15 occurences\n",
    "multi_cat_cols = ['brand_owner', 'brands', 'countries_en_first', 'packaging',\n",
    "                  'labels', 'traces_tags', 'allergens',\n",
    "                  'main_category_en', 'categories_en','pnns2', 'pnns1']\n",
    "def uniq() : return x.nunique()\n",
    "def min_occ(x) : return min(x.value_counts())\n",
    "def max_occ(x) : return max(x.value_counts())\n",
    "n = 15\n",
    "def less_th_n(x,n=n) : return (x.value_counts()<n).sum()\n",
    "def more_th_n(x,n=n) : return (x.value_counts()>=n).sum()\n",
    "\n",
    "display(df[multi_cat_cols].agg([uniq, min_occ, max_occ, less_th_n, more_th_n]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ser_m = ser.str.split(',')\n",
    "    flat_values = pd.Series([item for sublist in ser_m for item in sublist])\n",
    "    cat_occ = flat_values.value_counts()\n",
    "    to_del = cat_occ[cat_occ>n].index\n",
    "    for s in to_del:\n",
    "        ser_m = ser.str.replace(s, '')\n",
    "    ser_m = ser_m.apply(lambda x: tuple([s.strip() for s in x.split(',')\\\n",
    "                                         if s.strip()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest = pd.DataFrame({'col1':['aede, abcd', 'dsqk, dsk, a f', 'cck, 3 p']})\n",
    "\n",
    "for s in ['ab', 'ds', '3']:\n",
    "     dtest['col1'] = dtest['col1'].str.replace(s, '')\n",
    "\n",
    "dtest['col1'].apply(lambda x: tuple([s.strip() for s in x.split(',') if s.strip()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining groups of columns\n",
    "info_cols =  ['code', 'code_val', 'url', 'creator', 'created_datetime',\n",
    "               'product_name', 'ingredients_text']\n",
    "\n",
    "m_nutri_cols = ['fat_100g', 'carbohydrates_100g', 'proteins_100g',\n",
    "                'salt_100g', 'fiber_100g', 'saturated_fat_100g', 'sugars_100g']\n",
    "\n",
    "fat_cols = ['saturated_fat_100g', 'monounsaturated_fat_100g',\n",
    "            'polyunsaturated_fat_100g', 'omega_3_fat_100g',\n",
    "            '_alpha_linolenic_acid_100g', 'omega_6_fat_100g', \n",
    "            'trans_fat_100g', 'cholesterol_100g' ]\n",
    "\n",
    "carb_cols = ['_lactose_100g', 'starch_100g',  'sugars_100g', 'polyols_100g',\n",
    "              'fiber_100g', '_soluble_fiber_100g', '_insoluble_fiber_100g' ] \n",
    "\n",
    "oli_vit_cols = ['vitamin_a_100g', 'vitamin_d_100g', 'vitamin_e_100g',\n",
    "                'vitamin_k_100g', 'vitamin_c_100g', 'vitamin_b1_100g',\n",
    "                'vitamin_b2_100g', 'vitamin_pp_100g', 'vitamin_b6_100g',\n",
    "                'vitamin_b9_100g', 'folates_100g', 'vitamin_b12_100g',\n",
    "                'biotin_100g', 'pantothenic_acid_100g', 'bicarbonate_100g',\n",
    "                'potassium_100g', 'chloride_100g', 'calcium_100g',\n",
    "                'phosphorus_100g', 'iron_100g', 'magnesium_100g', 'zinc_100g',\n",
    "                'copper_100g', 'manganese_100g', 'selenium_100g', 'iodine_100g']\n",
    "\n",
    "addit_cols_cat = ['quantity_type', 'quantity_unit', 'packaging_main',\n",
    "                  'labels_main', 'allergens_main','traces_main','additives_main']\n",
    "\n",
    "addit_cols_num = ['quantity_num', 'ing_from_palm_oil_n', 'additives_n','fruit_veg_nut_100g']\n",
    "\n",
    "scores_cols = ['nutriscore', 'nutrigrade']\n",
    "\n",
    "cat_cols = ['main_category', 'categories_main','pnns2', 'pnns1', 'nova_group',\n",
    "            'brands_main', 'countries_main']\n",
    "\n",
    "misc_cols = ['alcohol_100g',  'caffeine_100g', 'cocoa_100g', 'sodium_100g']\n",
    "\n",
    "energy_cols = ['energy_from_fat_kj_100g', 'energy_kj_100g']\n",
    "\n",
    "tuple_cols = ['categories_tuple', 'brands_tuple','countries_tuple',\n",
    "              'packaging_tuple','additives_tuple','labels_tuple',\n",
    "              'allergens_tuple', 'traces_tuple']\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Defining groups of columns\n",
    "# info_cols =  ['code','code_val','url','creator','created_datetime',\n",
    "#               'product_name', 'ingredients_text']\n",
    "\n",
    "# m_nutri_cols = ['fat_100g','carbohydrates_100g','proteins_100g','sodium_100g',\n",
    "#                 'salt_100g','fiber_100g','saturated_fat_100g','sugars_100g']\n",
    "\n",
    "# addit_cols_cat = ['quantity_type','quantity_unit', 'packaging_main',\n",
    "#                   'serving_quantity','labels_main','allergens_main',\n",
    "#                   'traces_main','additives_main']\n",
    "\n",
    "# addit_cols_num = ['quantity_num', 'ing_from_palm_oil_n', 'additives_n']\n",
    "\n",
    "# scores_cols = ['nutriscore', 'nutrigrade']\n",
    "\n",
    "# cat_cols = ['main_category', 'categories_main','pnns2', 'pnns1', 'nova_group',\n",
    "#             'brands_main', 'countries_main']\n",
    "\n",
    "# energy_cols = ['energy_from_fat_kj_100g', 'energy_kj_100g']\n",
    "\n",
    "# tuple_cols = ['categories_tuple', 'brands_tuple','countries_tuple',\n",
    "#               'packaging_tuple','additives_tuple','labels_tuple',\n",
    "#               'allergens_tuple', 'traces_tuple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Temporary\n",
    "# # tirets\n",
    "# new_cols = [c.replace('-','_') for c in df.columns]\n",
    "# df = df.rename(columns=dict(zip(df.columns,new_cols)))\n",
    "# #nova\n",
    "# df['nova_group'] = df['nova_group'].astype('object')\n",
    "# # palm oil\n",
    "# palm = ['ingredients_from_palm_oil_n', 'ingredients_that_may_be_from_palm_oil_n']\n",
    "# df['ing_from_palm_oil_n'] = df[palm].sum(axis=1)\n",
    "# # fruits veg nuts\n",
    "# df['fruit_veg_nut_100g'] = df['fruits_vegetables_nuts_100g']\\\n",
    "#                      .combine_first(df['fruits_vegetables_nuts_estimate_100g'])\n",
    "# del df['fruits_vegetables_nuts_100g'], df['fruits_vegetables_nuts_estimate_100g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting score of knn depending on k\n",
    "fig, ax1 = plt.subplots(1)\n",
    "ax1.errorbar(list(res['param_n_neighbors']), res['mean_test_score'],\n",
    "             yerr=2*res['std_test_score'], lw=1.5, capsize = 4,\n",
    "             label='mean test score')\n",
    "ax1.errorbar(list(res['param_n_neighbors']), res['mean_train_score'],\n",
    "             yerr=2*res['std_train_score'], lw=1.5, capsize = 4,\n",
    "             label='mean train score')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(list(res['param_n_neighbors']), res['mean_fit_time'],\n",
    "             label='mean fit time', c='k', lw=1.5, ls='-.')\n",
    "ax1.set_ylabel('score'), ax2.set_ylabel('time')\n",
    "ax1.set_xlabel('k value')\n",
    "ax2.set(ylim=(0.2,0.3))\n",
    "ax1.legend(loc=1), ax2.legend(loc=2)\n",
    "plt.title('GridSearchCV result with knn',\n",
    "          fontweight='bold', fontsize=12)\n",
    "fig.text(0.4,0.65,'Best score: {:.3f}'.format(best_test_score)+\\\n",
    "               '\\nBest param: '+str(gsCV.best_params_))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "# -- Selecting most correlated features (see ppscore matrix below)\n",
    "cat_to_impute = 'main_category'\n",
    "vars_knn = ['categories_main']\n",
    "# ---- data with no nan\n",
    "# n_sample = \n",
    "df_tr = df[vars_knn+[cat_to_impute]].dropna(how='any')\n",
    "print('total nb of notna rows available to train the model:', df_tr.shape[0])\n",
    "# df_tr = df_tr.sample(n_sample)\n",
    "# print('selection of rows to train the model:', df_knn.shape[0])\n",
    "\n",
    "# -- Keeping only a subset with less unique values\n",
    "# ---- creating the subset\n",
    "n_uni_thresh = 500\n",
    "print(\"original 'categories_main': \", str(df_knn['categories_main'].nunique()), \"unique values.\")\n",
    "cat_occ = df['categories_main'].value_counts()\n",
    "to_keep = cat_occ[cat_occ>n_uni_thresh].index\n",
    "sub_ser = df['categories_main'].where(df['categories_main'].isin(to_keep))\n",
    "print(\"subset of 'categories_main': \", str(sub_ser.nunique()), \"unique values.\")\n",
    "# ---- visualizing to choose a threshold of unique values\n",
    "cat_occ.plot.bar(color='None', ec='r', lw=3)\n",
    "ax = plt.gca()\n",
    "ax.set(xlim=(15,50), ylim=(0,2500))\n",
    "ax.set_title(\"value_counts of 'categories_main'\", fontweight='bold')\n",
    "labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "short_labels = [s[0:7]+'.' if len(s)>7 else s for s in labels]\n",
    "ax.axes.set_xticklabels(short_labels)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.gcf().set_size_inches(8,2)\n",
    "plt.show()\n",
    "\n",
    "# Encoding dependant variables to dummy variables\n",
    "df_X_enc = pd.get_dummies(df_tr.loc[sub_ser.index])\n",
    "# ---> X to fit the model\n",
    "X = df_X_enc[vars_knn].values\n",
    "# ---> X where y missing rows\n",
    "ind_to_impute = df.loc[df[cat_to_impute].isna()].index\n",
    "X_ = df_X_enc.loc[ind_to_impute,vars_knn].values\n",
    "print('X shape (train): ', X.shape)\n",
    "print('X_ shape (to impute): ', X_.shape)\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df_knn.iloc[:,-1].values)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######## BROUILLON ###########\n",
    "# df_X = df.loc[indTT,model_cols]\n",
    "# print(df_X.shape, df_X.isna().sum().sum())\n",
    "# # replacing bugging gb.transform(lambda x: x.fillna(mean(x)))\n",
    "# med_tab = []\n",
    "# for n, gr in df_X.groupby('pnns1'):\n",
    "#     median_gr = gr.agg(np.nanmedian)\n",
    "#     med_tab.append(median_gr.isna().sum())\n",
    "#     for c in gr.columns[1:]:\n",
    "#         df_X.loc[gr.index,c] = df_X.loc[gr.index,c].fillna(median_gr[c])\n",
    "# display(df_X.sample(10), median_gr.isna().sum(), df_X.iloc[:,:].isna().sum())\n",
    "# print(med_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "# -- Selecting most correlated features (see ppscore matrix below)\n",
    "cat_to_impute = 'main_category'\n",
    "vars_knn = ['categories_main']\n",
    "# ---- data with no nan\n",
    "# n_sample = \n",
    "df_tr = df[vars_knn+[cat_to_impute]].dropna(how='any')\n",
    "print('total nb of notna rows available to train the model:', df_tr.shape[0])\n",
    "# df_tr = df_tr.sample(n_sample)\n",
    "# print('selection of rows to train the model:', df_knn.shape[0])\n",
    "\n",
    "# -- Keeping only a subset with less unique values\n",
    "# ---- creating the subset\n",
    "n_uni_thresh = 500\n",
    "print(\"original 'categories_main': \", str(df_tr['categories_main'].nunique()), \"unique values.\")\n",
    "cat_occ = df_tr['categories_main'].value_counts()\n",
    "to_keep = cat_occ[cat_occ>n_uni_thresh].index\n",
    "sub_ser = df_tr['categories_main'].where(df['categories_main'].isin(to_keep))\n",
    "print(\"subset of 'categories_main': \", str(sub_ser.nunique()), \"unique values.\")\n",
    "# ---- visualizing to choose a threshold of unique values\n",
    "cat_occ.plot.bar(color='None', ec='r', lw=3)\n",
    "ax = plt.gca()\n",
    "ax.set(xlim=(15,50), ylim=(0,2500))\n",
    "ax.set_title(\"value_counts of 'categories_main'\", fontweight='bold')\n",
    "labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "short_labels = [s[0:7]+'.' if len(s)>7 else s for s in labels]\n",
    "ax.axes.set_xticklabels(short_labels)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.gcf().set_size_inches(8,2)\n",
    "plt.show()\n",
    "df_tr_sel = df_tr.loc[sub_ser.index]\n",
    "print('remaining notna rows available to train the model:', df_tr_sel.shape[0])\n",
    "\n",
    "# Encoding dependant variables to dummy variables\n",
    "df_X_enc = pd.get_dummies(df_tr_sel[vars_knn])\n",
    "# ---> X to fit the model\n",
    "X = df_X_enc.values[:,:-1] # excluding 1 dummy to avoid multicolinearity\n",
    "# ---> X where y missing rows\n",
    "ind_to_impute = df.loc[df[cat_to_impute].isna()].index\n",
    "############ A REGLER ###############\n",
    "# X_ = df_X_enc.loc[ind_to_impute,vars_knn].values\n",
    "# print('X shape (train): ', X.shape)\n",
    "# print('X_ shape (to impute): ', X_.shape)\n",
    "# le = LabelEncoder()\n",
    "# y = le.fit_transform(df_knn.iloc[:,-1].values)\n",
    "# X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing existing model\n",
    "gsCV, res = joblib.load('My Drive/--DATA SCIENCE/PROJET3/gsCV_res_C2.pkl')\n",
    "gsCV, res = joblib.load('/content/gsCV_res_C2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting the model\n",
    "joblib.dump((gsCV, res),'/content/gsCV_res_C2.pkl');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessing(df.dropna(subset=var_model+[var_target]),\n",
    "                              var_model=var_model, var_target=var_target,\n",
    "                              enc_strat_cat=enc_strat_cat)\n",
    "\n",
    "## Data Preprocessing\n",
    "var_model = ['carbohydrates_100g','nutriscore','energy_kj_100g'] # ,'pnns1','pnns2'\n",
    "X, y = data_preprocessing(df.dropna(subset=var_model+['fiber_100g']),\n",
    "                          var_model=var_model,\n",
    "                          var_target='fiber_100g', enc_strat_cat='label')\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2)\n",
    "## Training KNN regressor\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "gsCV = GridSearchCV(KNeighborsRegressor(),\n",
    "                    {'n_neighbors': [3,5,7,9,11,13]},\n",
    "                    cv=kf, return_train_score=True)\n",
    "gsCV.fit(X_tr, y_tr)\n",
    "res = gsCV.cv_results_\n",
    "## Predicting test set with the model and forcing in [0,100]\n",
    "y_pr = gsCV.predict(X_te).clip(0,100)\n",
    "# Comparison with naive baselines\n",
    "naive_model_compare_r2(X_tr, y_tr, X_te, y_te, y_pr)\n",
    "\n",
    "## Predicting using knn regressor\n",
    "# Selecting rows with missing values in 'fiber_100g'\n",
    "df_miss = df.loc[df['fiber_100g'].isna()]\n",
    "X_, y_ = data_preprocessing(df_miss, var_model=var_model,\n",
    "                          var_target='fiber_100g', enc_strat_cat='label')\n",
    "# Predicting with model\n",
    "y_pr_ = gsCV.predict(X_)\n",
    "# Plotting histogram of predicted values\n",
    "plot_hist_pred_val(y_te, y_pr, y_pr_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preprocessing\n",
    "var_model = ['categories_main','carbohydrates_100g', 'proteins_100g',\n",
    "             'sugars_100g','fat_100g', 'nutriscore','energy_kj_100g']\n",
    "X, y = data_preprocessing(df.dropna(subset=var_model+['main_category']).sample(1000),\n",
    "                          var_model=var_model, var_target='main_category',\n",
    "                          enc_strat_cat='label')\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2)\n",
    "## Training KNN Classifier\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "gsCV = GridSearchCV(KNeighborsClassifier(),\n",
    "                    {'n_neighbors': [7,9,11,13]},\n",
    "                    cv=skf, return_train_score=True,\n",
    "                    scoring='f1_weighted')\n",
    "gsCV.fit(X_tr, y_tr)\n",
    "res = gsCV.cv_results_\n",
    "## Predicting test set with the model\n",
    "y_pr = gsCV.predict(X_te)\n",
    "# Comparison with naive baselines\n",
    "naive_model_compare_acc_f1(X_tr, y_tr, X_te, y_te, y_pr)\n",
    "\n",
    "## Predicting using knn\n",
    "# Selecting rows with missing values in 'fiber_100g'\n",
    "df_miss = df.loc[df['main_category'].isna()]\n",
    "X_, y = data_preprocessing(df_miss, var_model=var_model,\n",
    "                          var_target='main_category', enc_strat_cat='label')\n",
    "# Predicting with model\n",
    "y_pr_ = gsCV.predict(X_)\n",
    "Plotting histogram of predicted values\n",
    "plot_hist_pred_val(y_te, y_pr, y_pr_, short_lab=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for rows where sum of components exceeds total\n",
    "\n",
    "print('****  SUB-SUB COLUMNS  ****')\n",
    "print(\"Nb of rows where sum sub-sub-columns > sub-column:\\n\",'--'*25)\n",
    "# ---- sugars_100g\n",
    "m_sug_out = df['sugars_100g']*1.2<(df[sug_cols].sum(axis='columns'))\n",
    "print(\"'sugars_100g'\", df[m_sug_out][sug_cols+['sugars_100g']].shape[0])\n",
    "# ---- fiber_100g\n",
    "m_fib_out = df['fiber_100g']*1.2<(df[fib_cols].sum(axis='columns'))\n",
    "print(\"'fiber_100g'\", df[m_fib_out][fib_cols+['fiber_100g']].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting sub-sub-columns to NaN where (XXX_sum > XXX) \n",
    "\n",
    "df.loc[:,sug_cols] = df[sug_cols].where(~m_sug_out, other=np.nan)\n",
    "df.loc[:,fib_cols] = df[fib_cols].where(~m_fib_out, other=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for rows where sum of components exceeds total\n",
    "\n",
    "print('****  SUB COLUMNS  ****')\n",
    "# ---- fat_100g\n",
    "m_fat_out = df['fat_100g']<(df[fat_cols].sum(axis='columns'))\n",
    "print(\"'fat_100g'\", df[m_fat_out].shape[0])\n",
    "# ---- carbohydrates_100g\n",
    "m_carb_out = df['carbohydrates_100g']<(df[carb_cols].sum(axis='columns'))\n",
    "print(\"'carbohydrates_100g'\", df[m_carb_out].shape[0])\n",
    "# ---- salt_100g\n",
    "m_salt_out = df['salt_100g']>(df[salt_cols].sum(axis='columns')/0.387)\n",
    "print(\"'sodium_100g'\", df[m_salt_out].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting sub-columns to NaN where (XXX_sum > XXX) \n",
    "\n",
    "df.loc[:,fat_cols] = df[fat_cols].where(~m_fat_out, other=np.nan)\n",
    "df.loc[:,carb_cols] = df[carb_cols].where(~m_carb_out, other=np.nan)\n",
    "df.loc[:,salt_cols] = df[salt_cols].where(~m_salt_out, other=np.nan)\n",
    "\n",
    "df.loc[m_salt_out,'sodium_100g'] = df['salt_100g']*0.388"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for rows where sum of components > 100\n",
    "\n",
    "m_out = df[sum_100g_cols].sum(axis='columns')>100\n",
    "m_0 = df[sum_100g_cols].sum(axis='columns')==0\n",
    "m_0_100 = df[sum_100g_cols].sum(axis='columns').between(0,100)\n",
    "sum_100g = df[sum_100g_cols].sum(axis='columns')\n",
    "hist = plt.hist(sum_100g, color='b', bins=500, label=('0 < sum < 100'))\n",
    "ind = np.argmax(hist[1]>=100)\n",
    "ax = plt.gca()\n",
    "w = hist[1][1]-hist[1][0]\n",
    "ax.bar(hist[1][ind:-1]+w/2, hist[0][ind:], ec = 'r',\n",
    "       lw = 2, width=w,color='None', label=('sum > 100'))\n",
    "ax.set(xlim=(-5,125), ylim=(0,30000))\n",
    "fig = plt.gcf()\n",
    "ax.set_ylabel('number of rows')\n",
    "fig.suptitle('Histogram - Sum of components (values for 100g)',\n",
    "             fontweight='bold', fontsize=14)\n",
    "fig.set_size_inches(12,3)\n",
    "plt.savefig(os.getcwd()+'/FIG/C_2_4_hist_sum_100g.png', dpi=400)\n",
    "plt.show()\n",
    "print('\\n****  TOTAL MAIN COLUMNS  ****')\n",
    "print(\"nb of rows where sum > 100: \", df[sum_100g_cols][m_out].shape[0])\n",
    "print(\"nb of rows where sum = 0: \", df[sum_100g_cols][m_0].shape[0])\n",
    "print(\"nb of rows where 0 > sum > 100: \", df[sum_100g_cols][m_0_100].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
